"use strict";(self.webpackChunkkratix_docs=self.webpackChunkkratix_docs||[]).push([[8130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"promise-best-practises","metadata":{"permalink":"/blog/promise-best-practises","source":"@site/blog/2025-11-12-best-practises-for-writing-promises/index.mdx","title":"Best practices for Promise-writing","description":"Best practises to bear in mind when writing your Promises","date":"2025-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"workflows","permalink":"/blog/tags/workflows"},{"inline":true,"label":"promises","permalink":"/blog/tags/promises"}],"readingTime":13.12,"hasTruncateMarker":true,"authors":[{"name":"Sapphire Mason-Brown","title":"Engineer @ Syntasso","url":"https://github.com/saphmb","imageURL":"https://github.com/saphmb.png","key":"sapphire","page":null}],"frontMatter":{"slug":"promise-best-practises","title":"Best practices for Promise-writing","description":"Best practises to bear in mind when writing your Promises","authors":["sapphire"],"tags":["kratix","workflows","promises"]},"unlisted":false,"nextItem":{"title":"Introducing the Kratix Promise-writing SDKs: Faster, Easier, More Testable","permalink":"/blog/sdk-product-announcement"}},"content":"As platforms are so unique to organisations, and the services required by platform users are so vast, Kratix has always been very\\nflexible when it comes to the design of Promises. Other than the expectation that Promise writers honour the Promise schema, the\\nscope for how Promises can be designed is vast; containers can be written in any language, workflows can be as segmented as you\\nwould like and workflow actions can be imperative or declarative.\\n\\nHowever, there are a number design practices and approaches to Promise development that can make development and maintenance\\neasier for Promise developers and make consuming services via Promises better for users.\\n\\nIn this blog post, we\'re going design a Promise with some core fundamentals in mind, paving the way for improved debugging,\\nreliability and user clarity.\\n\\n\x3c!-- truncate --\x3e\\n\\nTo get started, we\'re going to bootstrap our Promise. We\'ll be creating a Redis Promise to provision Redis instances as-a-Service.\\nWe\'ll start by creating a new directory for our Promise and will use the [kratix cli](/main/kratix-cli/intro) to bootstrap it. Let\'s start by running the following commands:\\n\\n```bash\\nmkdir redis-promise\\n\\nkratix init promise redis \\\\\\n  --group mygroup.org \\\\\\n  --kind Redis \\\\\\n  --version v1alpha1\\n```\\n\\nThis creates a basic `promise.yaml` and an example resource request - `example-resource.yaml` - for our Promise. So far, this\\nPromise doesn\u2019t do much, let\u2019s leverage an existing Redis operator to create instances of our Redis. To do this, the Redis\\noperator, CRDs and other dependencies must exist on all destinations before the Promise can provision Redis instances.\\n\\nTo start, create a directory called `dependencies`:\\n\\n```bash\\nmkdir dependencies\\n```\\n\\nThen run the following to download the manifests for the redis dependencies and place them in the `dependencies` directory.\\n\\n```bash\\ncurl -L https://raw.githubusercontent.com/syntasso/kratix-marketplace/refs/heads/main/redis/internal/configure-pipeline/resources/dependencies/all-redis-operator-resources.yaml -o dependencies/operator-bundle.yaml\\n\\ncurl https://raw.githubusercontent.com/syntasso/kratix-marketplace/refs/heads/main/redis/internal/configure-pipeline/resources/dependencies/databases.spotahome.com_redisfailovers.yaml > dependencies/databases.spotahome.com_redisfailovers.yaml\\n```\\n\\nWe can now use the kratix cli to ensure these dependencies are installed when the Promise\u2019s Configure Pipeline runs with the\\n`update dependencies` command:\\n\\n```bash\\nkratix update dependencies dependencies --image ghcr.io/syntasso/redis-dependencies:v0.0.2\\n```\\n\\nThis command:\\n\\n- Places the dependencies in the `workflows/promise/configure/dependencies/configure-deps/resources` directory\\n- Creates a Dockerfile that copies the contents of the resource directory into the `/kratix/outputs` directory so they will be\\n  written to all appropriate destinations\\n- Adds the `promise.configure` step to the `pipeline.yaml`\\n\\nYour directory should now look something like this:\\n\\n```bash\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 dependencies\\n\u2502\xa0\xa0 \u251c\u2500\u2500 databases.spotahome.com_redisfailovers.yaml\\n\u2502\xa0\xa0 \u2514\u2500\u2500 operator-bundle.yaml\\n\u251c\u2500\u2500 example-resource.yaml\\n\u251c\u2500\u2500 promise.yaml\\n\u2514\u2500\u2500 workflows\\n    \u2514\u2500\u2500 promise\\n        \u2514\u2500\u2500 configure\\n            \u2514\u2500\u2500 dependencies\\n                \u2514\u2500\u2500 configure-deps\\n                    \u251c\u2500\u2500 Dockerfile\\n                    \u251c\u2500\u2500 resources\\n                    \u2502\xa0\xa0 \u251c\u2500\u2500 databases.spotahome.com_redisfailovers.yaml\\n                    \u2502\xa0\xa0 \u2514\u2500\u2500 operator-bundle.yaml\\n                    \u2514\u2500\u2500 scripts\\n                        \u2514\u2500\u2500 pipeline.sh\\n\\n9 directories, 9 files\\n```\\n\\nAnd the promise.yaml should include the below:\\n\\n```yaml\\n  workflows:\\n    config: {}\\n#highlight-start\\n    promise:\\n      configure:\\n      - apiVersion: platform.kratix.io/v1alpha1\\n        kind: Pipeline\\n        metadata:\\n          name: dependencies\\n        spec:\\n          containers:\\n          - image: ghcr.io/syntasso/redis-dependencies:v0.0.2\\n            name: configure-deps\\n#highlight-end\\n    resource: {}\\n```\\n\\nNow it is time to build the image that will run as part of the  `promise.configure` step. If you are running your cluster on kind,\\nyou can build and load the image to your cluster with:\\n\\n```bash\\ndocker build --tag ghcr.io/syntasso/redis-dependencies:v0.0.1 workflows/promise/configure/dependencies/configure-deps\\n\\nkind load docker-image ghcr.io/syntasso/redis-dependencies:v0.0.1 --name platform\\n```\\n\\nWe can now install this early iteration of the Promise!\\n\\n```bash\\nkubectl apply --filename promise.yaml\\n```\\n\\nYou will soon see the Redis dependencies appearing on the worker cluster. Watch the operator pod as it is created with:\\n\\n```bash\\nkubectl get pods --context kind-worker --watch\\n```\\n\\nYou should eventually see output similar to the following:\\n\\n```bash\\nNAME                             READY   STATUS    RESTARTS   AGE\\nredisoperator-7f584c6969-zc8sx   1/1     Running   0          41s\\n```\\n\\nYou now have a Promise that installs the Redis operator and its its dependencies but it does not yet provision Redis as-a-service\\n\\n**Declarative Workflows**\\n\\nWhilst it would have been possible to install the operator and it\u2019s dependencies with `kubectl apply` commands targeting the cluster,\\nwe have chosen a *declarative* approach; defining the desired state of the resources on the cluster and allowing the configured\\nGitOps tools to converge on this desired state. Taking a declarative approach improves reliability as GitOps tools automatically\\ncorrect drift, provide audibility as the configurations are declared up-front, and are more predictable. \\n\\nThis is also essential for ensuring that Kratix can easily delete the resources created when Promises are deleted as Kratix deletes\\nthe files created as part of any configure workflows during Promise deletion.\\n\\n:::info\\nThis is in-keeping with the \\"Declare and Converge\\" pattern used to manage Kubernetes objects.\\n\\nYou can read more about this in the [Kubernetes Documentation](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/)\\n:::\\n\\nAt the moment, our Promise configure workflow is declarative and defines all prerequisites for installing the Redis Operator on\\nthe cluster, including the CRD for the RedisFailover CR. With these installed, we can continue to to take a declarative approach\\nwith our resource configure workflows - that is, what happens when a user makes a request of our redis promise.\\n\\nWithin this workflow, we want to create a RedisFailover CR from the inputs provided by the user. There are a number of options\\nthat can be configured in the RedisFailover CR but there are some defaults that we, as the Promise writers, want to enforce. The\\nonly thing we want to users to be able to configure is the number of replicas and the name of their RedisFailover. \\n\\nWe\u2019ll take the name of the RedisFailover from the name of the user\u2019s requests and the number of replicas from the API exposed by\\nthe Promise. We\u2019re going to wrap the number of replicas in a simple abstraction where `small` is one replica and `large` is three;\\nwe can implement the logic for this in our workflow which we\u2019ll introduce shortly. \\n\\nFirst, we\u2019re going to update the Promise API to introduce the `size` property. We can do this with the `update api` command:\\n\\n```bash\\nkratix update api --property size:string\\n```\\n\\nYou should see that the following has been added to the `promise.yaml`\\n\\n```yaml\\n  api:\\n    apiVersion: apiextensions.k8s.io/v1\\n    kind: CustomResourceDefinition\\n    metadata:\\n      creationTimestamp: null\\n      name: rediss.mygroup.org\\n    spec:\\n      group: mygroup.org\\n      names:\\n        kind: Redis\\n        plural: rediss\\n        singular: redis\\n      scope: Namespaced\\n      versions:\\n      - name: v1alpha1\\n        schema:\\n          openAPIV3Schema:\\n            properties:\\n              spec:\\n                properties:\\n                  size:\\n                    type: string\\n                type: object\\n            type: object\\n        served: true\\n        storage: true\\n```\\n\\nWe can now add a workflow that uses to this property. Again, we can use the kratix cli - this time with the `add container`\\ncommand:\\n\\n```bash\\nkratix add container resource/configure/instance-configure --image ghcr.io/syntasso/redis-dependencies:v0.0.1 --language go\\n```\\n\\nThis will have updated the `promise.yaml` to add a `resource.configure` workflow and have bootstrapped a Docker image that can be\\nbuilt to run as part of this workflow. As we has specified that we want to use the [kratix-go sdk](https://pkg.go.dev/github.com/syntasso/kratix-go) for this Promise via the `--language`\\nflag, we also have a `pipeline.go` file. Update the script to contain the following:\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"fmt\\"\\n\\n\\tkratix \\"github.com/syntasso/kratix-go\\"\\n)\\n\\nfunc main() {\\n\\tsdk := kratix.New()\\n\\tresource, _ := sdk.ReadResourceInput()\\n\\tname := resource.GetName()\\n\\tsize, err := resource.GetValue(\\"spec.size\\")\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"failed fetch \'size\' property from request: %v\\", err)\\n\\t}\\n\\t\\n\\tvar replicas int\\n\\tif size == \\"small\\" {\\n\\t\\treplicas = 1\\n\\t}\\n\\t\\n\\tif size == \\"large\\" {\\n\\t\\treplicas = 3\\n\\t}\\n\\t\\n\\tfailoverTemplate := `apiVersion: databases.spotahome.com/v1\\nkind: RedisFailover\\nmetadata:\\n  name: %s\\n  namespace: default\\nspec:\\n  redis:\\n    imagePullPolicy: IfNotPresent\\n    replicas: %d\\n    customConfig:\\n      - \\"maxclients 100\\"\\n      - \\"hz 50\\"\\n      - \\"timeout 60\\"\\n      - \\"tcp-keepalive 60\\"\\n      - \\"client-output-buffer-limit normal 0 0 0\\"\\n      - \\"client-output-buffer-limit slave 1000000000 1000000000 0\\"\\n      - \\"client-output-buffer-limit pubsub 33554432 8388608 60\\"\\n    resources:\\n      limits:\\n        cpu: 400m\\n        memory: 500Mi\\n      requests:\\n        cpu: 100m\\n        memory: 100Mi\\n  sentinel:\\n    imagePullPolicy: IfNotPresent\\n    replicas: %d\\n    customConfig:\\n      - \\"down-after-milliseconds 2000\\"\\n      - \\"failover-timeout 3000\\"\\n    resources:\\n      limits:\\n        memory: 100Mi\\n      requests:\\n        cpu: 100m\\n`\\n\\t\\tredisContent := []byte(fmt.Sprintf(failoverTemplate, name, replicas, replicas))\\n\\t\\n\\t\\tsdk.WriteOutput(\\"redis_failover.yaml\\", redisContent)\\n}\\n```\\n\\nWe\u2019re going to execute this script when creating and updating a resource request. It will:\\n\\n1. Retrieve the name and size of the Redis from the resource request\\n2. Interpolate these in the template for the generated redis failover alongside the configured defaults\\n3. Write the RedisFailover CR to the `/kratix/output` directory so it can be written to a destination.\\n\\nAs this workflow is written declaratively, there will be no attempts to create the Redis after initial creation. It will only be\\nupdated if there is a change in the specification of the resource.\\n\\n**Idempotency**\\n\\nIdempotency refers to the ability to run an action multiple times with the guarantee that there will be no additional effect of\\nrunning after it the initial action. In our case, if the resource configure workflow runs a second or third time, no additional\\nRedis instance will be created on these occasions. This is part of what having a declarative workflow enables. As the workflow\\ndescribes the *desired* Redis instance, if the workflow re-runs for any reason, the Redis will not be created again. It will\\nhowever, be updated if there are any changes to the request.\\n\\n**Error Handling**\\n\\nWhen writing Promise workflows, correct error handling is essential and we must ensure that, should an error occur, the container\\nexits with a non-zero exit code.\\n\\nTake a look at this part of our script:\\n\\n```bash\\n\\tredisContent := []byte(fmt.Sprintf(failoverTemplate, name, size, size))\\n\\tsdk.WriteOutput(\\"redis_failover.yaml\\", redisContent)\\t\\n```\\n\\nThe go SDK\u2019s `WriteOutput` function returns an error if for any reason, we are unable to write output to a file. However, at\\npresent, we are not checking if an error arises and do not exit if it does. In this form, the write could fail, the container\\nwould exit and we wouldn\u2019t notice that something had gone wrong until the RedisFailover failed to appear on the destination.\\n\\nLet\u2019s mitigate against exactly this, update the workflow with the following:\\n\\n```bash\\n\\tredisContent := []byte(fmt.Sprintf(failoverTemplate, name, size, size))\\n\\t\\n\\terr = sdk.WriteOutput(\\"redis_failover.yaml\\", redisContent)\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"failed to write output: %v\\", err)\\n\\t}\\n```\\n\\nThe script will now exit with an error if one is returned by the `WriteOutput` function. Now, if the write fails it is immediately obvious from the execution of the workflow pod as it will fail.\\n\\n**Informing users via the status**\\n\\nWhen users make a request, as Promise writers we can be as specific as we would like about the information we would like to\\nsurface to users about their request. For some resources, this can include details about how to access a given service with a url\\nor database string for instance. We\u2019re going to do something more simple, when the `redis_failover.yaml` has been successfully\\nwritten to the outputs, we\u2019re going to inform users that it is in the \u201cProvisioning\u201d stage so they know that their request is in\\nprogress.\\n\\nWe can do this with the SDK. Add the following to the end of the workflow script:\\n\\n```bash\\n\\tstatus := kratix.NewStatus()\\n\\tstatus.Set(\\"stage\\", \\"provisioning\\")\\n\\tif err := sdk.WriteStatus(status); err != nil {\\n\\t\\tlog.Fatalf(\\"failed to write status: %v\\", err)\\n\\t}\\n```\\n\\nThis part of the workflow:\\n\\n1. Creates a status \\n2. Sets the key and value `stage: provisioning` on the status\\n3. Writes the status as a `status.yaml` file to the `metadata` directory\\n\\nWhen the `status.yaml` file is written to the metadata directory, Kratix will automatically persist that information to the status\\nof the Resource. \\n\\nThis  status-writing happens at the end of the workflow, but what if there is a failure earlier in the workflow? How can we inform\\nthe user that something has gone wrong? In these error cases, we can update the status by patching the resource directly via the\\nAPI.\\n\\nLet\u2019s revisit the previous example of error handling:\\n\\n```bash\\n\\tredisContent := []byte(fmt.Sprintf(failoverTemplate, name, size, size))\\n\\t\\n\\terr := sdk.WriteOutput(\\"redis_failover.yaml\\", redisContent)\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"failed to write output: %v\\", err)\\n\\t}\\n```\\n\\nIf the `WriteOutput` function fails, we want to ensure that an error in provisioning is surfaced in the status of the requested resource. Update the script with the following:\\n\\n```bash\\n\\tredisContent := []byte(fmt.Sprintf(failoverTemplate, name, size, size))\\n\\n\\terr := sdk.WriteOutput(\\"redis_failover.yaml\\", redisContent)\\n\\tif err != nil {\\n#highlight-start\\n\\t\\tstatus := kratix.NewStatus()\\n\\t\\tstatus.Set(\\"stage\\", \\"provisioning error\\")\\n\\t\\tif err := sdk.PublishStatus(resource, status); err != nil {\\n\\t\\t\\tlog.Fatalf(\\"failed to publish status: %v\\", err)\\n\\t\\t}\\n#highlight-end\\n\\t\\tlog.Fatalf(\\"failed to write output: %v\\", err)\\n\\t}\\n```\\n\\nIf an error occurs when writing the output, before exiting the container we are now:\\n\\n1. Creating a status and setting the `stage` to `provisioning error`\\n2. Calling `PublishStatus` to update the resource directly. The `PublishStatus` function performs a patch of the resource.\\n\\nThis ensures that even in scenarios where errors occur, we still communicate the status of the resource to users.\\n\\n<details>\\n<summary>\\n\\n_Click here_ for the full `pipeline.go` file.\\n\\n</summary>\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"fmt\\"\\n\\t\\"log\\"\\n\\n\\tkratix \\"github.com/syntasso/kratix-go\\"\\n)\\n\\nfunc main() {\\n\\tsdk := kratix.New()\\n\\tresource, _ := sdk.ReadResourceInput()\\n\\tname := resource.GetName()\\n\\tsize, err := resource.GetValue(\\"spec.size\\")\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"failed fetch \'size\' property from request: %v\\", err)\\n\\t}\\n\\n\\tvar replicas int\\n\\tif size == \\"small\\" {\\n\\t\\treplicas = 1\\n\\t}\\n\\n\\tif size == \\"large\\" {\\n\\t\\treplicas = 3\\n\\t}\\n\\n\\tfailoverTemplate := `apiVersion: databases.spotahome.com/v1\\nkind: RedisFailover\\nmetadata:\\n  name: %s\\n  namespace: default\\nspec:\\n  redis:\\n    imagePullPolicy: IfNotPresent\\n    replicas: %d\\n    customConfig:\\n      - \\"maxclients 100\\"\\n      - \\"hz 50\\"\\n      - \\"timeout 60\\"\\n      - \\"tcp-keepalive 60\\"\\n      - \\"client-output-buffer-limit normal 0 0 0\\"\\n      - \\"client-output-buffer-limit slave 1000000000 1000000000 0\\"\\n      - \\"client-output-buffer-limit pubsub 33554432 8388608 60\\"\\n    resources:\\n      limits:\\n        cpu: 400m\\n        memory: 500Mi\\n      requests:\\n        cpu: 100m\\n        memory: 100Mi\\n  sentinel:\\n    imagePullPolicy: IfNotPresent\\n    replicas: %d\\n    customConfig:\\n      - \\"down-after-milliseconds 2000\\"\\n      - \\"failover-timeout 3000\\"\\n    resources:\\n      limits:\\n        memory: 100Mi\\n      requests:\\n        cpu: 100m\\n`\\n\\n\\tredisContent := []byte(fmt.Sprintf(failoverTemplate, name, replicas, replicas))\\n\\n\\terr = sdk.WriteOutput(\\"redis_failover.yaml\\", redisContent)\\n\\tif err != nil {\\n\\t\\tstatus := kratix.NewStatus()\\n\\t\\tstatus.Set(\\"stage\\", \\"provisioning error\\")\\n\\t\\tif err := sdk.PublishStatus(resource, status); err != nil {\\n\\t\\t\\tlog.Fatalf(\\"failed to publish status: %v\\", err)\\n\\t\\t}\\n\\t\\tlog.Fatalf(\\"failed to write output: %v\\", err)\\n\\t}\\n\\n\\tstatus := kratix.NewStatus()\\n\\tstatus.Set(\\"stage\\", \\"provisioning\\")\\n\\tif err := sdk.WriteStatus(status); err != nil {\\n\\t\\tlog.Fatalf(\\"failed to write status: %v\\", err)\\n\\t}\\n}\\n\\n```\\n\\n</details>\\nWe are now ready to build this image and make our first resource request for a redis-as-a-service. \\n\\nLet\u2019s start by initialising the go modules for the script and building the image:\\n\\n```bash\\npushd workflows/resource/configure/instance-configure/ghcr-io-syntasso-redis-configure/scripts\\n\\tgo mod init syntasso/redis-configure\\ngo mod tidy\\npopd\\n\\ndocker build --tag ghcr.io/syntasso/redis-configure:v0.0.1 workflows/resource/configure/instance-configure/ghcr-io-syntasso-redis-configure/\\n\\nkind load docker-image ghcr.io/syntasso/redis-configure:v0.0.1 --name platform\\n```\\n\\nWith the image built and loaded in our environment, we are now ready to make a resource request.\\n\\nUpdate the `example-resource.yaml` to specify a size:\\n\\n```bash\\napiVersion: mygroup.org/v1alpha1\\nkind: Redis\\nmetadata:\\n  name: example-redis\\n#highlight-start\\nspec:\\n  size: small\\n#highlight-end\\n```\\n\\nNow we can use the `kubectl` cli to apply this request:\\n\\n```bash\\nkubectl apply --file example-resource.yaml\\n```\\n\\nShortly, we should begin to see the Redis pods starting on the worker cluster. Let\'s check with:\\n\\n```bash\\nkubectl get pods --context kind-worker\\n```\\n\\nYou should see something similar to the following:\\n\\n```bash\\nNAMESPACE            NAME                                           READY   STATUS    RESTARTS       AGE\\ndefault              redisoperator-7f584c6969-7hh9z                 1/1     Running   0              10m\\ndefault              rfr-example-redis-0                            1/1     Running   0              75s\\ndefault              rfs-example-redis-79c8b8c977-fdv9x             1/1     Running   0              75s\\n```\\n\\nAdditionally, the resource request should detail that it is in a `provisioning` state. Run:\\n\\n```bash\\nkubectl describe redis example-redis\\n```\\n\\nThe status should show the following:\\n\\n```bash\\n  Last Successful Configure Workflow Time:  2025-11-11T14:40:07Z\\n  Message:                                  Resource requested\\n  Observed Generation:                      1\\n#highlight-start\\n  Stage:                                    provisioning\\n#highlight-end\\n  Workflows:                                1\\n  Workflows Failed:                         0\\n  Workflows Succeeded:                      1 \\n```\\n\\nCongratulations! You have written a Promise that employs some of the best practices for Promise implementation; declarative\\nworkflows, idempotency, correct error handling, and effective use of the resource status. With these in mind, you\u2019ll create\\nPromises that are easier to debug and update, and provide a better experience for consumers.\\n\\nIf you\'ve got more questions about how to write your Promises, don\'t hesitate to reach out to us and the Kratix community via\\nthe [Kratix Community Slack](https://kratixworkspace.slack.com/)! We\'d love to hear from you."},{"id":"sdk-product-announcement","metadata":{"permalink":"/blog/sdk-product-announcement","source":"@site/blog/2025-08-18-sdk-product-announcement/index.mdx","title":"Introducing the Kratix Promise-writing SDKs: Faster, Easier, More Testable","description":"New Kratix Promise-writing SDKs in Python and Golang","date":"2025-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"product announcement","permalink":"/blog/tags/product-announcement"},{"inline":true,"label":"sdks","permalink":"/blog/tags/sdks"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"golang","permalink":"/blog/tags/golang"}],"readingTime":4.03,"hasTruncateMarker":true,"authors":[{"name":"Cat Morris","title":"Product Manager @ Syntasso","url":"https://github.com/catmo-syntasso","imageURL":"https://github.com/catmo-syntasso.png","key":"cat","page":null}],"frontMatter":{"slug":"sdk-product-announcement","title":"Introducing the Kratix Promise-writing SDKs: Faster, Easier, More Testable","description":"New Kratix Promise-writing SDKs in Python and Golang","authors":["cat"],"tags":["kratix","product announcement","sdks","python","golang"]},"unlisted":false,"prevItem":{"title":"Best practices for Promise-writing","permalink":"/blog/promise-best-practises"},"nextItem":{"title":"Introducing the SKE Headlamp Plugin","permalink":"/blog/introducing-ske-headlamp"}},"content":"import ReactPlayer from \'react-player/lazy\';\\n```mdx-code-block\\nimport SDKContract from \\"./sdk-contract.png\\"\\n```\\n\\n:::danger[New feature? WOW!] \\nThese SDKs excel at everything from bootstrapping the initial Promise for your organisation\'s platform to ensuring consistency and testability across all the Promises your platform teams create.\\n:::\\n\\nDefining and building platform APIs and services is no easy task in general. Platform engineers who adopt Kratix and \\ncreate bespoke services \u2013 which we call [\u201cPromises\u201d](https://docs.kratix.io/main/reference/promises/intro)  \u2013 often \\nencounter a significant \\"wall of bash\\" when starting their journey towards taming their underlying IaC config, YAML, and API calls. \\n\\n\x3c!-- truncate --\x3e\\nOur community has shared the challenges of bootstrapping custom Promise workflows with us. They often resort to bash scripts \\nfor simplicity, only to hit the proverbial brick wall when adding more functionality, running tests, or managing repetitive \\ncode for common tasks like validation or file I/O interactions.\\n\\nThis is why we are thrilled to announce the launch of our brand-new Promise-writing Software Development Kits (SDKs) in \\n[Python](https://github.com/syntasso/kratix-python) and [Golang](https://github.com/syntasso/kratix-go). These completely \\nremove the complexity of common Promise-specific interactions for every team writing promises. \\nAvailable to everyone in the Kratix community, these SDKs also serve as a template for teams to create their own SDKs in the \\nlanguage of their choice and share them with the community!\\n\\n## Why Create Promise-writing SDK for Kratix?\\n\\nWe believe it should be easy to write Promise stages in the language of your choosing, focusing only on the business logic \\nunique to your organisation and Promise. No more rewriting everything in a more testable language three months down the line, \\nonce you realise bash doesn\u2019t quite cut it.\\n\\nOur new Promise-writing SDK directly tackles those pains head-on. It empowers platform engineers to focus solely on the unique \\nbusiness logic that truly matters to your organisation in a language that your team knows like the back of their hands. This \\nSDK simplifies those common, repetitive tasks that previously required significant manual coding, such as:\\n- **Handling file I/O interactions and validation:** Crucial for ensuring data integrity and simplifying the process of writing \\nto output directories, ensuring anything you output will be in the right format.\\n- **Reading Promise and Resource inputs:** The SDK automatically parses YAML inputs, dynamically loading custom inputs as object \\nproperties, completely eliminating manual coding for Promise developers.\\n- **Clear, mockable interfaces:** Promises can now be unit tested via these interfaces, improving feedback cycles and reducing \\nrisk to your platform when new promises are added.\\n\\n\\n## Extending the Kratix CLI: SDKs, Specifications, and Contracts\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={SDKContract} alt=\\"Screenshot of the Kratix Promise-writing SDK Contract\\" />\\n  <figcaption>Some of the functions available in the Python and Golang SDKs</figcaption>\\n</figure>\\n\\nThe SDK is built around a clear, open-source contract. It is written in Markdown and can be found in \\nthe [Kratix repo](https://github.com/syntasso/kratix/blob/main/sdk/contract.md). This contract isn\'t \\njust a guide; it\'s a specification to encourage and support community contributions of SDKs in various languages, \\nensuring consistency and parity across implementations. Each SDK will live in its own open-source repository, making \\nit simple for the community to consume and contribute.\\n\\nAnd finally, to make things even easier, we\u2019ve integrated these SDKs into our \\n[Promise-writing CLI](https://docs.kratix.io/main/kratix-cli/intro), meaning you can pre-load your generated Promises \\nwith pipeline stages in Go or Python. In the video below, our team shows how to use the CLI to build your own, \\nbespoke Python promise in under 3 minutes!\\n\\n<div style={{\\"text-align\\":\\"center\\", \\"margin\\":\\"10px 0\\"}}>\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/Q817NcogqK0?si=cMyRk61vtg0k1w9m\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n</div>\\n\\nThe Promise-writing SDK is a significant step towards reducing the time it takes to write Promises, \\nwhich massively increases their testability, too! We\u2019re excited to see the community rally around this \\ninitiative, and to support them, we\'ll be working hard to integrate examples and documentation into the \\nKratix CLI and our workshops, making the onboarding experience unbelievably easy.\\n\\n## Get Hands-on with the Kratix SDKs: Feedback is Welcomed!\\n\\nJoin our [Slack community](https://kratixworkspace.slack.com/), where we\u2019ve created a channel for the \\nPromise-writing SDKs (creatively named #promise-writing-sdks) to get feedback, share ideas and support \\ncommunity members who want to build one in the language of their choice. If you want to get hands-on right \\naway, check out the [Python](https://github.com/syntasso/kratix-python) and [Go](https://github.com/syntasso/kratix-go) \\nSDK repositories on GitHub, or the contract in the [Kratix.io](https://github.com/syntasso/kratix) repo and let us know your thoughts!"},{"id":"introducing-ske-headlamp","metadata":{"permalink":"/blog/introducing-ske-headlamp","source":"@site/blog/2025-07-11-introducing-headlamp/index.mdx","title":"Introducing the SKE Headlamp Plugin","description":"introducing the SKE Headlamp plugin","date":"2025-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"SKE","permalink":"/blog/tags/ske"},{"inline":true,"label":"headlamp","permalink":"/blog/tags/headlamp"}],"readingTime":4.2,"hasTruncateMarker":true,"authors":[{"name":"Derik Evangelista","title":"Engineer @ Syntasso","url":"https://github.com/kirederik","imageURL":"https://2.gravatar.com/avatar/7ac63fbda18c97f6a7fab8af157021367793187f4c5830eb722ff565c5a767e9?size=256","key":"derik","page":null}],"frontMatter":{"slug":"introducing-ske-headlamp","title":"Introducing the SKE Headlamp Plugin","description":"introducing the SKE Headlamp plugin","authors":["derik"],"tags":["kratix","SKE","headlamp"]},"unlisted":false,"prevItem":{"title":"Introducing the Kratix Promise-writing SDKs: Faster, Easier, More Testable","permalink":"/blog/sdk-product-announcement"},"nextItem":{"title":"New in Kratix: Health Checks and Progressive Rollout","permalink":"/blog/healthcheck-progressive-rollout"}},"content":"Navigating Kubernetes resources can often feel like searching for a needle in a\\nhaystack, especially when trying to gain a comprehensive overview of everything\\ndeployed within your cluster. While kubectl is a familiar tool, we\'ve heard that\\nit can be complicated to visualise the entire landscape of Kratix Promises and\\nResources.\\n\\nTo address this, we\'ve been developing a powerful new solution: a SKE plugin\\nfor Headlamp, designed to provide unparalleled observability for your Kubernetes\\nenvironments.\\n\\n{/* truncate */}\\n\\n## Why Headlamp?\\n\\n[Headlamp](https://headlamp.dev/) is a user-friendly Kubernetes UI focused on\\nextensibility and its core purpose is to offer robust observability on top of\\nKubernetes. It provides adaptable UI & branding to create custom\\nexperiences with minimal effort and features RBAC-based controls, adapting to a\\nuser\'s cluster permissions. It can be run as a web app, desktop app, or\\nin-cluster. Headlamp is a Cloud Native Computing Foundation project, part of the\\nKubernetes SIG UI.\\n\\nWhile the SKE Backstage integration provides a layer of observability, it is\\nmuch more aimed at the Platform users, not as much to the Platform teams\\nbuilding the Promises powering the Platform. Headlamp, on the other hand, offers\\na similar experience to Backstage, but much more relevant to the Platform teams.\\n\\nCrucially, Headlamp supports the development of custom plugins, and that\'s\\nprecisely what we\'ve leveraged to create our Kratix plugin.\\n\\n## Enhanced Visibility with the Kratix Headlamp Plugin\\n\\nOur new plugin integrates seamlessly with your Headlamp installation, offering a\\nuser-friendly interface to manage and monitor your Kratix resources. Watch the\\ndemo below or keep on reading for a closer look at its key features.\\n\\n```mdx-code-block\\nimport ReactPlayer from \'react-player/lazy\';\\nimport HeadlampDemo from \'./headlamp-overview.mp4\';\\n```\\n\\n<ReactPlayer wrapper=\\"span\\" controls url={HeadlampDemo} />\\n\\n### The Promise Views\\n\\nThis dedicated tab provides a quick overview of all your Kratix Promises. From\\nhere, you can easily jump into the details of any specific Promise, such as a\\nRedis Promise, and view further information. Within the Promise details, you can\\naccess crucial information, including:\\n\\n* The conditions of the Promise.\\n* The last time the associated pipeline ran.\\n* The ability to jump directly into the pipeline run itself.\\n* Visibility into the pods that were executed as part of that pipeline.\\n* Access to logs for detailed troubleshooting.\\n\\n```mdx-code-block\\nimport PromiseList from \\"./promise-list.png\\"\\nimport PromiseDetail from \\"./promise-details.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={PromiseList} alt=\\"The Promise List view\\" />\\n\\n  <figcaption>The Promise List view</figcaption>\\n</figure>\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={PromiseDetail} alt=\\"The Promise Details view\\" />\\n\\n  <figcaption>The Promise Details view</figcaption>\\n</figure>\\n\\n\\n### The Resource Views\\n\\nComplementing the Promises view, the Resources tab allows you to see all the\\nKratix resources deployed across your cluster. The plugin offers powerful\\nfiltering capabilities, enabling you to narrow down your view by, for example,\\nPromise or Namespace\\n\\nJust like with Promises, you can click on any resource (e.g., an \'app cache\' in\\nthe mobile namespace) to view its details. This includes seeing when its\\npipeline last ran, jumping into the pipeline, inspecting executed pods, and\\ninvestigating logs for all containers involved in the execution.\\n\\n```mdx-code-block\\nimport ResourceList from \\"./resource-list.png\\"\\nimport ResourceDetail from \\"./resource-details.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={ResourceList} alt=\\"The Resource List view\\" />\\n\\n  <figcaption>The Resource List view</figcaption>\\n</figure>\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={ResourceDetail} alt=\\"The Resource Details view\\" />\\n\\n  <figcaption>The Resource Details view</figcaption>\\n</figure>\\n\\n\\n### The Map View: A Visual Overview\\n\\nHeadlamp includes a very useful built-in visualisation where you can see all\\nresources deployed across your cluster. We\'ve extended this capability to\\nprovide a similar map view specifically for your Kratix Promises. By filtering\\nfor Kratix resources, you can visually explore your deployed Promises and\\nunderstand their relationships.\\n\\nFor instance, you can click on a Promise to see the resources it owns, like\\nits Resource Requests. Furthermore, the map view highlights associated\\njobs, such as the pipeline jobs that executed the Promise. You can also see the\\nPromise\'s configure pipeline directly in this tree-like, map-based view,\\nallowing for easy navigation and understanding of your platform\'s entire\\nresource topology.\\n\\n```mdx-code-block\\nimport MapView from \\"./map-view.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={MapView} alt=\\"The Map View view\\" />\\n\\n  <figcaption>The Map view</figcaption>\\n</figure>\\n\\n## Try it out!\\n\\n:::info\\n\\nThe Kratix Headlamp plugin is currently available for Syntasso Kratix Enterprise\\n(SKE) users.\\n\\n:::\\n\\nWe\'re excited about the potential of this new plugin to dramatically simplify\\nKubernetes observability for Kratix users.\\n\\nLooking ahead, our next major focus is to introduce comprehensive visibility\\ninto compound promises, allowing you to easily see their sub-promises and\\nassociated resources. This will provide an even deeper understanding of your\\nPlatform.\\n\\nAlso, we are keen to get your feedback! What information would you like us to\\nexpose next? What\'s missing? What are some features you\'d like to see? Send the\\nrequests our way!\\n\\nIf you\'re interested in trying out a version of Headlamp with our plugin, please\\nlet us know! We can package it up and send it your way."},{"id":"healthcheck-progressive-rollout","metadata":{"permalink":"/blog/healthcheck-progressive-rollout","source":"@site/blog/2025-05-27-healthchecks-progressive-rollout/index.mdx","title":"New in Kratix: Health Checks and Progressive Rollout","description":"Progressive rollout with healthcheck","date":"2025-05-27T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"SKE","permalink":"/blog/tags/ske"},{"inline":true,"label":"healthcheck","permalink":"/blog/tags/healthcheck"},{"inline":true,"label":"upgrade","permalink":"/blog/tags/upgrade"}],"readingTime":5.15,"hasTruncateMarker":true,"authors":[{"name":"Chunyi Lyu","title":"Engineer @ Syntasso","url":"https://github.com/ChunyiLyu","imageURL":"https://github.com/ChunyiLyu.png","key":"chunyi","page":null},{"name":"Derik Evangelista","title":"Engineer @ Syntasso","url":"https://github.com/kirederik","imageURL":"https://2.gravatar.com/avatar/7ac63fbda18c97f6a7fab8af157021367793187f4c5830eb722ff565c5a767e9?size=256","key":"derik","page":null}],"frontMatter":{"slug":"healthcheck-progressive-rollout","title":"New in Kratix: Health Checks and Progressive Rollout","description":"Progressive rollout with healthcheck","authors":["chunyi","derik"],"tags":["kratix","SKE","healthcheck","upgrade"]},"unlisted":false,"prevItem":{"title":"Introducing the SKE Headlamp Plugin","permalink":"/blog/introducing-ske-headlamp"},"nextItem":{"title":"Speeding up local dev: fast feedback when building pipelines","permalink":"/blog/local-dev-pipelines"}},"content":"Earlier this year, we introduced an exciting new capability in Kratix: [health checks for resources](/main/guides/resource-health).\\nThis addition allows platform teams and app developer to easily observe the status of their requested workloads, without the need to switch context and find it in Destinations.\\n\\nIn this blog, we\u2019ll discuss how you can use it to support progressive rollouts when updating Promises.\\n\\n## What\u2019s the Problem?\\n\\nWhen a Promise gets updated, say with a new version of a Helm chart, the standard behavior in Kratix is to reconcile and update all resource requests at once.\\nThat\u2019s fine in simple dev environments, but for complex workloads, upgrading everything at once is risky.\\nA failed update could disrupt many environments simultaneously, and debugging becomes difficult when failures are scattered.\\n\\nPlatform engineers need a safer approach: progressive rollouts. Instead of deploying changes to your entire fleet at once,\\nprogressive rollout allows teams to introduce updates gradually to limit the impact, gather early feedback,\\nand catch potential bugs before releasing broadly.\\nBut for that to work, Kratix needs a way to understand the health of each individual resource during and after an upgrade.\\n\\n{/* truncate */}\\n\\n## Health Checks to the Rescue\\n\\nPlatforms are often described as a black box - you need to integrate dozens of tools into complicated data ingestion\\nand dashboard platforms to understand what is happening with all your resources.\\nFor a long time, Platforms orchestrated by Kratix were no different. The new health check feature enables Promise authors\\nto define custom health validation logic for the resources their Promise provisions, which means not only can you report\\nback basic health data (ready, degraded, failed etc.) but also context relevant that can be used to drive other processes,\\nfor example connection strings to access your database resource, or the unique ID name of the application you\'ve just deployed.\\n\\nHere\u2019s how it works:\\n\\nYou can simply update your existing Promise resource configure workflow to output a [HealthDefinition](/ske/reference/healthdefinition),\\nwhich contains structured instructions on how to perform health checks against requested resources in a Destination or external system.\\nKratix reads the health status and automatically updates the status of each resource request.\\n\\nBy including a HealthDefinition, you give Kratix insight into the runtime state of each resource, whether it\u2019s ready, degraded, or failed.\\nWith better visibility into Resource health, Promise authors now have the foundation for more complex orchestration, such as progress rollout.\\n\\n```mdx-code-block\\nimport HCDiagram from \\"/img/docs/ske/guide-healthcheck-arch.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={HCDiagram} alt=\\"High-level diagram of how Health Check works in Kratix\\" />\\n\\n  <figcaption>How Health Checks works in SKE</figcaption>\\n</figure>\\n\\nHealth Definition includes all the necessary information to execute the Health Check workflow in a Destination.\\nAs part of the [Syntasso Kratix Enterprise](/ske) offering, we provide a [SKE Health Agent](https://docs.kratix.io/ske/kratix/ske-health-agent) that\\ncan reconcile Health Definition in a Kubernetes Destination.\\nIf you\'re not using SKE, you will need to bring your own agent to act on Health Definitions.\\nYou can also refer to our [Surfacing health information](/main/guides/resource-health) guide on how to run health checks\\nwithout an agent in the Platform Cluster.\\n\\n## Progressive Rollouts in Action\\nLet\u2019s say your Promise provisions PostgreSQL as a service. You\u2019re installing a new version of the Promise that updates some of it default configurations.\\n\\nIn your PostgreSQL Promise resource configure workflow, you can include a container that can\\n1. output a HealthDefinition to check Postgres health\\n1. use Kubernetes Leases to ensure it only continues the workflow when it acquires the lease\\n\\nWith Health Checks enabled and set up in your Platform and Destinations, when you update the Promise:\\n1. Kratix begins updating resource requests\\n1. One of the Resource Configure workflow acquires the Lease\\n1. Lease is only released after Health checks report a healthy status, ensuring Resources are upgraded one by one\\n1. When there\'s a failed upgrade, Lease are never released and rollout is paused and the failed state is visible in the resource request status\\n1. Kratix continues upgrading Resources one by one until all of them are upgraded and reported healthy\\n\\nThis progressive delivery strategy has a myriad of benefits:\\n1. Mitigate risk and downtime\\nBy gradually exposing changes to a small percentage of users, you limit the blast radius of bugs or failures.\\nIf something goes wrong, the impact is contained and easier to recover from.\\n2. Improve debuggability\\nSmaller rollouts make it significantly easier to isolate and troubleshoot issues.\\nYou\u2019re not drowning in signals from a full-scale deployment, so you can focus on specific failure cases as they arise.\\n3. Fast feedback loops for experimental features.\\nYou can gather real-world data on performance metrics, error rates, and user feedback from a limited rollout,\\nand use them to validate assumptions before proceeding further.\\n4. Increased trust in your platform\\nStakeholders: developers, operators, and other users, will gain confidence in the platform when they see that changes\\nare deployed safely and reliably, with safeguards in place to detect and respond to issues.\\n\\n## What\'s next\\n\\nWith the introduction of health checks in Kratix, platform engineers and Promise authors now have a powerful tool to drive safer, smarter, and more observable upgrades.\\nBy embedding health logic directly into resource workflows, Kratix can track the status of each individual resource and gate rollout progression accordingly.\\n\\nThis unlocks the ability to perform progressive rollouts - updating one resource at a time, validating its health before proceeding.\\nThis dramatically reduces the risk of widespread failures and makes it easier to pinpoint and debug issues when they occur.\\nIt also brings greater confidence and control to platform teams managing complex workloads across multiple environments.\\n\\nIf you\u2019re looking to improve the reliability and maintainability of your service updates, health checks are a foundational step forward.\\nCheck out our [step-by-step guide](/ske/guides/healthchecks) to get started.\\nWe are working to add more Kratix native functionalities to make progressive upgrades easier for Promise authors, and are always keen to hear your thoughts and feedback.\\nLet us know how you\u2019re using them\u2014we\u2019d love to hear from you in our Slack or GitHub communities."},{"id":"local-dev-pipelines","metadata":{"permalink":"/blog/local-dev-pipelines","source":"@site/blog/2025-04-22-local-dev-pipelines/index.mdx","title":"Speeding up local dev: fast feedback when building pipelines","description":"Learn how to speed up local development with fast feedback loops when building pipelines.","date":"2025-04-22T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"promises","permalink":"/blog/tags/promises"},{"inline":true,"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":10.81,"hasTruncateMarker":true,"authors":[{"name":"Aslan Avci","title":"Engineer @ Syntasso","url":"https://github.com/alparslanavci","imageURL":"https://github.com/alparslanavci.png","key":"aslan","page":null},{"name":"Derik Evangelista","title":"Engineer @ Syntasso","url":"https://github.com/kirederik","imageURL":"https://2.gravatar.com/avatar/7ac63fbda18c97f6a7fab8af157021367793187f4c5830eb722ff565c5a767e9?size=256","key":"derik","page":null}],"frontMatter":{"slug":"local-dev-pipelines","title":"Speeding up local dev: fast feedback when building pipelines","description":"Learn how to speed up local development with fast feedback loops when building pipelines.","authors":["aslan","derik"],"tags":["kratix","promises","pipelines"]},"unlisted":false,"prevItem":{"title":"New in Kratix: Health Checks and Progressive Rollout","permalink":"/blog/healthcheck-progressive-rollout"},"nextItem":{"title":"Kratix, Backstage, and OIDC","permalink":"/blog/backstage-and-keycloak"}},"content":"Are you building Pipelines and unsure about the best way to test and iterate over them? Worry not. Your questions are about to be answered!\\n\\nIn this blog post, you will gain insights into how you can get faster feedback on your Pipeline development by:\\n\\n- Running the pipeline stage locally\\n- Running automated tests\\n- Testing imperative actions\\n- Running the stage as a container\\n\\nWe will also look into how to more effectively run system-level tests by\\n\\n- Populating the image cache\\n- Setting the image pull policy\\n- Automating end-to-end (e2e) tests\\n\\nClick \\"read more\\" below to continue!\\n\\n{/* truncate */}\\n\\n---\\n\\n## Testing your Pipeline stages\\n\\nAs you may know, a Promise includes a set of workflows that are executed as part of a request for that Promise. The workflow itself is a series of Pipelines, and each Pipeline contains multiple Stages. These stages are usually encapsulated in a Container image.\\n\\nAt the start of a Pipeline Stage, Kratix will provide, in the `/kratix/input` directory, a YAML file (`object.yaml`) representing the user\'s resource request. This file is the same object that the user has applied to the Platform cluster.\\n\\n:::tip\\n\\nWe will only cover building and testing a Resource Request Pipeline stage in this post. A Promise Pipeline Stage has slightly different inputs, but the process to build and test is the same. Check the [Workflow Reference docs](/main/reference/workflows) for more details.\\n\\n:::\\n\\nKratix expects the Stage to produce outputs, and those outputs are stored in either `/kratix/output` or `/kratix/metadata` (depending on the purpose; not clear? Read the [Workflows reference docs](/main/reference/workflows#volumes)).\\n\\nOne of the main benefits of Kratix is the ability to test these Stages. In this blog post, let\'s assume we are building the following Pipeline stage:\\n\\n- It takes as input an object containing `spec.contents`.\\n- It produces a ConfigMap with the `spec.contents`.\\n\\nIn Python, this stage may look like this:\\n\\n```python\\n#!/usr/bin/env python3\\nimport yaml, sys\\n\\nwith open(\\"/kratix/input/object.yaml\\") as f:\\n    obj = yaml.safe_load(f)\\n\\nname = obj.get(\\"metadata\\", {}).get(\\"name\\", \\"example-config\\")\\ncontents = obj.get(\\"spec\\", {}).get(\\"contents\\", {})\\n\\nconfigmap = {\\n    \\"apiVersion\\": \\"v1\\",\\n    \\"kind\\": \\"ConfigMap\\",\\n    \\"metadata\\": {\\"name\\": name, \\"namespace\\": \\"default\\"},\\n    \\"data\\": contents\\n}\\n\\nwith open(\\"/kratix/output/configmap.yaml\\", \\"w\\") as out:\\n    yaml.dump(configmap, out)\\n```\\n\\nPipeline Stages can be tested at different levels. In the next sections, we will explore ways you could implement and test at different levels, from unit to end-to-end tests. The diagram below illustrate some of the testing strategies you could use.\\n\\n```mdx-code-block\\nimport Figure01 from \\"./figure01.jpg\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure01} alt=\\"A diagram showing the different stages a pipeline stage can be tested\\" />\\n\\n  <figcaption>Testing strategies for a pipeline stage</figcaption>\\n</figure>\\n\\n\\n### Running the pipeline stage locally\\n\\nThe simplest place to start is to make it runnable locally. That means you will be able to execute the script and quickly verify its output.\\n\\nIf we try to execute our Python script above, it will fail. It\'s relying on the Kratix Volumes, which (most likely) are not available on your machine.\\n\\nA simple strategy is to parameterise the inputs and outputs. For example, you could update the script to read the volumes from environment variables, defaulting to the Kratix volumes:\\n\\n```python ,title=\\"generate_configmap.py\\"\\n#!/usr/bin/env python3\\nimport yaml, sys, os\\n\\n# highlight-start\\ninput_path = os.getenv(\\"INPUT_PATH\\") or \\"/kratix/input/object.yaml\\"\\noutput_path = os.getenv(\\"OUTPUT_PATH\\") or \\"/kratix/output/configmap.yaml\\"\\n\\nwith open(input_path) as f:\\n    obj = yaml.safe_load(f)\\n# highlight-end\\n\\nname = obj.get(\\"metadata\\", {}).get(\\"name\\", \\"example-config\\")\\ncontents = obj.get(\\"spec\\", {}).get(\\"contents\\", {})\\n\\nconfigmap = {\\n    \\"apiVersion\\": \\"v1\\",\\n    \\"kind\\": \\"ConfigMap\\",\\n    \\"metadata\\": {\\"name\\": name, \\"namespace\\": \\"default\\"},\\n    \\"data\\": contents\\n}\\n\\n# highlight-start\\nwith open(output_path, \\"w\\") as out:\\n    yaml.dump(configmap, out)\\n# highlight-end\\n```\\n\\nWith this simple change, you can now run the script locally, passing the input and output as environment variables. First, create an example input file:\\n\\n```bash\\nmkdir {input,output}\\ncat <<EOF > input/object.yaml\\nmetadata:\\n  name: configtest\\nspec:\\n  contents:\\n    keyOne: one\\n    keyTwo: two\\nEOF\\n```\\n\\nNext, save the Python script in a file (call it `generate_configmap.py`) and make it executable. Then, set the environment variables and execute the script:\\n\\n:::tip\\n\\nYou may need to install `pyyaml` to execute the script below.\\n\\n```bash\\npip3 install pyyaml\\n```\\n\\n:::\\n\\n```bash\\nINPUT_PATH=input/object.yaml OUTPUT_PATH=output/configmap.yaml ./generate_configmap.py\\n```\\n\\nThis should produce an `output-example.yaml` locally that matches your expectations:\\n\\n```yaml\\n$ cat output-example.yaml\\napiVersion: v1\\ndata:\\n  keyOne: one\\n  keyTwo: two\\nkind: ConfigMap\\nmetadata:\\n  name: configtest\\n  namespace: default\\n```\\n\\nThis is a start, but we can do better.\\n\\n### Running automated tests\\n\\nRunning locally is great, but it doesn\'t allow us to quickly verify if our script is working. If you look closely, you\'ll notice that a stage is not that different from a normal function in most programming languages: it transforms inputs into outputs.\\n\\nTherefore, it should be quite simple to unit test these using whatever libraries your language of choice provides. Let\'s do that with our script.\\n\\nFirst, let\'s refactor it to make it more testable by extracting the main logic into a function:\\n\\n```python ,title=\\"generate_configmap.py\\"\\n#!/usr/bin/env python3\\nimport yaml, sys, os\\n\\ndef generate_configmap(input_path=None, output_path=None):\\n    input_path = input_path or os.getenv(\\"INPUT_PATH\\") or \\"/kratix/input/object.yaml\\"\\n    output_path = output_path or os.getenv(\\"OUTPUT_PATH\\") or \\"/kratix/output/configmap.yaml\\"\\n\\n    with open(input_path) as f:\\n        obj = yaml.safe_load(f)\\n\\n    name = obj.get(\\"metadata\\", {}).get(\\"name\\", \\"example-config\\")\\n    contents = obj.get(\\"spec\\", {}).get(\\"contents\\", {})\\n\\n    configmap = {\\n        \\"apiVersion\\": \\"v1\\",\\n        \\"kind\\": \\"ConfigMap\\",\\n        \\"metadata\\": {\\"name\\": name, \\"namespace\\": \\"default\\"},\\n        \\"data\\": contents\\n    }\\n\\n    with open(output_path, \\"w\\") as out:\\n        yaml.dump(configmap, out)\\n\\nif __name__ == \\"__main__\\":\\n    generate_configmap()\\n```\\n\\nNext, let\'s create a basic unit test file (called `test_generate_configmap.py`) for it:\\n\\n```python ,title=\\"test_generate_configmap.py\\"\\n#!/usr/bin/env python3\\nimport unittest\\nimport yaml\\nimport tempfile\\nimport os\\nfrom generate_configmap import generate_configmap\\n\\nclass TestGenerateConfigMap(unittest.TestCase):\\n    def test_configmap_generation(self):\\n        input_data = {\\n            \\"metadata\\": {\\"name\\": \\"test-config\\"},\\n            \\"spec\\": {\\"contents\\": {\\"foo\\": \\"bar\\", \\"baz\\": \\"qux\\"}}\\n        }\\n\\n        with tempfile.NamedTemporaryFile(mode=\\"w+\\", delete=False) as tmp_in:\\n            yaml.dump(input_data, tmp_in)\\n            tmp_in_path = tmp_in.name\\n\\n        with tempfile.NamedTemporaryFile(mode=\\"r\\", delete=False) as tmp_out:\\n            tmp_out_path = tmp_out.name\\n\\n        try:\\n            generate_configmap(input_path=tmp_in_path, output_path=tmp_out_path)\\n\\n            with open(tmp_out_path) as f:\\n                configmap = yaml.safe_load(f)\\n\\n            self.assertEqual(configmap[\\"kind\\"], \\"ConfigMap\\")\\n            self.assertEqual(configmap[\\"metadata\\"][\\"name\\"], \\"test-config\\")\\n            self.assertEqual(configmap[\\"data\\"], {\\"foo\\": \\"bar\\", \\"baz\\": \\"qux\\"})\\n\\n        finally:\\n            os.remove(tmp_in_path)\\n            os.remove(tmp_out_path)\\n\\nif __name__ == \\"__main__\\":\\n    unittest.main()\\n```\\n\\nYou should now be able to run the tests:\\n\\n```bash\\n$ python3 test_generate_configmap.py\\n.\\n----------------------------------------------------------------------\\nRan 1 test in 0.005s\\n\\nOK\\n```\\n\\nAs you can see, this is a very simple use case and test. However, your pipeline stage may be complex, with several branches in the code. Adding a unit test suite like the above can help you move faster by ensuring you\'re not accidentally breaking existing features.\\n\\n:::tip\\n\\nIt\'s very common for Promise Writers to start with `bash` for their pipeline stages. While here at Syntasso we are #TeamBash, stages will often evolve into their own beasts, together with the scripts that back them. Consider the language you choose for your stages carefully, as some make it much harder to test than others \ud83d\ude09.\\n\\n:::\\n\\n### Testing imperative actions\\n\\nA common use case is for the Stage to execute imperative actions, like calling APIs to validate or create external resources. At the unit level, you could easily add tests for those cases by using whatever features/libraries your language provides for mocking those external calls. For example, in Python, you could use the built-in [unittest.mock](https://docs.python.org/3/library/unittest.mock.html).\\n\\nThe takeaway is that your stage is _just software_. You can use the same tools and methodologies that you use to build other pieces of software to build the stages. Over time, you will build a suite of unit, integration, and system tests around your stages to guarantee their functionality.\\n\\n### Running the Stage as a Container\\n\\nOnce you have your test suite running locally, you may want to execute a test in your container engine to guarantee it works in a containerised environment. In our example, we can run it in Docker by creating the following Dockerfile:\\n\\n```Dockerfile\\nFROM python:3.10-slim\\n\\nWORKDIR /app\\n\\n# Copy the script into the container\\nCOPY generate_configmap.py .\\n\\n# Install PyYAML\\nRUN pip install --no-cache-dir pyyaml\\n\\n# Run the script\\nCMD [\\"python\\", \\"generate_configmap.py\\"]\\n```\\n\\nWe can then build it:\\n\\n```bash\\ndocker build -t configmap-generator:dev .\\n```\\n\\nTo execute it, you will need to ensure the container has access to the expected Kratix Volumes. You can use the `--volume` flag to mount a local directory into the container at the specified path. To run the container, run:\\n\\n```bash\\ndocker run --rm \\\\\\n  --volume $(pwd)/input:/kratix/input \\\\\\n  --volume $(pwd)/output:/kratix/output \\\\\\n  configmap-generator:dev\\n```\\n\\nExecuting the command above will mount your local `input` and `output` directories as the Kratix Volumes. You should see the generated output in the `output` directory. You can play around and change the inputs to verify it further.\\n\\n## Testing the Promise\\n\\nGreat! At this point, you are confident that your stage performs as expected, and you\'re ready to add it to your Promise. In this section, let\'s explore how to validate your stages in an actual Promise.\\n\\n### Populating the image cache\\n\\nIf you just add your stage to the Promise and install it, you will likely get an `ErrImagePull` error. That\'s because Kubernetes will try to download the image from the actual registry.\\n\\nYou could build and push your image to the registry, apply your Promise, test, and repeat; however, that can be quite error-prone and slow.\\n\\nIf you are running Kratix in a local Kubernetes cluster, deployed via KinD or Minikube, for example, a simpler approach is to load the container image directly into the Kubernetes node cache.\\n\\nIf using KinD, you can run:\\n\\n```bash\\nkind load docker-image configmap-generator:dev --name platform\\n```\\n\\nNote that we are tagging the image with a `dev` tag. We recommend you set a tag in your container image: when it\'s not set (or when it\'s `latest`), extra configuration is required for Kubernetes to use the correct cached image.\\n\\n### Setting image pull policy\\n\\nBy default, the Kratix Pipeline Pod containers will have the `imagePullPolicy` set to the defaults that [Kubernetes defines](https://kubernetes.io/docs/concepts/containers/images/#imagepullpolicy-defaulting). In development, you often want that to be either `IfNotPresent` or `Never`. That is, Kubernetes should only attempt to download from the registry when it\'s not already in the cache (or never).\\n\\nImage Pull Policy is a Stage-level configuration: you set it per container in your Pipeline, and different stages may have a different pull policy. For example:\\n\\n```yaml\\napiVersion: platform.kratix.io/v1alpha1\\nkind: Promise\\nmetadata:\\n  name: postgresql\\nspec:\\n  api: # ...\\n  workflows:\\n    resources:\\n      configure:\\n        - apiVersion: platform.kratix.io/v1alpha1\\n          kind: Pipeline\\n          metadata:\\n            name: instance\\n          spec:\\n            containers:\\n              - name: stageOne\\n                image: configmap-generator:dev\\n                #highlight-next-line\\n                imagePullPolicy: Never\\n              - name: stageTwo\\n                image: another-stage:v1.0.0\\n                #highlight-next-line\\n                imagePullPolicy: Always\\n```\\n\\nIf you are making changes to your stage but are not actually seeing the results you expect, double-check you are using the image you actually want!\\n\\n### Automating e2e tests\\n\\nYou will also want to test the full end-to-end result of executing the Pipeline. While the steps above gave you confidence that each individual stage will work in isolation, you still need the full pipeline execution to validate whether they all, together, produce the expected result.\\n\\nThe simplest way to test this is to run something akin to a system test:\\n\\n1. Install the Promise:\\n   - Verify the expected dependencies are installed in the Destination.\\n1. Request a Resource from the Promise:\\n   - Verify the expected resources are created in the Destination.\\n\\nIn fact, this is how we test all the Promises in our Marketplace. For example, the [Redis Promise](https://github.com/syntasso/kratix-marketplace/blob/main/redis/internal/scripts/test) executes the following tests on CI:\\n\\n```bash\\n#!/usr/bin/env bash\\nset -e\\n\\ntest_promise() {\\n  kubectl wait promise/redis --for=condition=ConfigureWorkflowCompleted --timeout=120s\\n  kubectl get crd redisfailovers.databases.spotahome.com\\n  kubectl wait --for=condition=Available --timeout=5s deployment/redisoperator\\n}\\n\\ntest_resource_request() {\\n  kubectl wait --for=condition=Available --timeout=5s deployment/rfs-example\\n}\\n\\nif [ \\"$1\\" = \\"promise\\" ]; then\\n  test_promise\\nelse\\n  test_resource_request\\nfi\\n```\\n\\nOn Promise installation, it expects a few resources to appear on the Destination, including the Redis Operator deployment. On Resource Request, it expects a new deployment for the Redis instance to appear on the Destination.\\n\\n:::tip\\n\\nThe test script above assumes that the Destination kube context is the one currently targeted.\\n\\n:::\\n\\nYou can use a similar approach to validate that all of the stages work as expected. This is especially important when stages can modify the outputs from a previous stage (like the `/kratix/metadata/status.yaml` file).\\n\\n## Conclusion\\n\\nBuilding and testing Pipelines in Kratix doesn\'t have to be a trial-and-error process. By treating your stages as standard software components\u2014backed by tests\u2014you gain faster feedback, greater confidence, and smoother iteration.\\n\\nThe more you invest in testability, the faster you can move from idea to production-ready Promise. Whether you\'re crafting a simple configuration generator or orchestrating complex external systems, the techniques covered here help you ship robust, dependable Pipelines.\\n\\nWe hope this post gives you a good base on how to develop your pipelines. As always, feel free to drop by our [Slack](https://kratix.slack.com) or [GitHub](https://github.com/syntasso/kratix) to continue the conversation.\\n\\nHappy shipping! \ud83d\ude80"},{"id":"backstage-and-keycloak","metadata":{"permalink":"/blog/backstage-and-keycloak","source":"@site/blog/2025-04-16-backstage-keycloak/index.mdx","title":"Kratix, Backstage, and OIDC","description":"Integrating Kratix with Backstage and Keycloak","date":"2025-04-16T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"backstage","permalink":"/blog/tags/backstage"},{"inline":true,"label":"oidc","permalink":"/blog/tags/oidc"}],"readingTime":15.78,"hasTruncateMarker":true,"authors":[{"name":"Derik Evangelista","title":"Engineer @ Syntasso","url":"https://github.com/kirederik","imageURL":"https://2.gravatar.com/avatar/7ac63fbda18c97f6a7fab8af157021367793187f4c5830eb722ff565c5a767e9?size=256","key":"derik","page":null}],"frontMatter":{"slug":"backstage-and-keycloak","title":"Kratix, Backstage, and OIDC","description":"Integrating Kratix with Backstage and Keycloak","authors":["derik"],"tags":["kratix","backstage","oidc"]},"unlisted":false,"prevItem":{"title":"Speeding up local dev: fast feedback when building pipelines","permalink":"/blog/local-dev-pipelines"},"nextItem":{"title":"Debugging in Kratix","permalink":"/blog/debugging-promise-updates"}},"content":"The newly released version of the SKE Backstage plugins will no longer rely on a Git repository to perform CRUD operations on the Kubernetes Cluster. Instead, they will now use the Kubernetes API to manage the resources directly. This enables users to get the most up-to-date information on their resources, as well as manage resources created via other means, like via `kubectl`.\\n\\nOne important aspect is that you can now use the plugins with OIDC providers, allowing you to have finer control over the authentication and authorisation process.\\n\\nIn this blog, we\'ll go through the process of setting up your Kubernetes Cluster with Keycloak, and configuring Backstage to use it for authentication. We will then configure the SKE Backstage plugins to use the OIDC token provided by Keycloak.\\n\\n{/* truncate */}\\n\\n## Deploying Keycloak\\n\\nThe first thing you will need is a Keycloak instance. Setting one up can be quite a complicated process, and a bit out of scope for this blog.\\n\\nFor simplicity, we will be using a hosted Keycloak instance from [cloud-iam.com](https://cloud-iam.com/). You can sign up for a free account and get a Keycloak instance up and running in minutes. Once deployed, take note of the `realm`. You will need it on the next steps.\\n\\n:::tip\\n\\nIf you deploy your Keycloak instance with cloud-iam.com, you should receive the admin password in the email sent to you once your Deployment is ready.\\n\\n:::\\n\\n### Creating the resources\\n\\nNow that you have your Keycloak instance up and running, you will need to set up the clients for Backstage and Kubernetes. You can go through the UI and manually create them, or you can use the [Keycloak Terraform Provider](https://registry.terraform.io/providers/keycloak/keycloak/latest). You will need:\\n\\n- A Client for Backstage and a Client for Kubernetes\\n- Users that can login to both Backstage and Kubernetes\\n- The Groups that will be used to control access to the Kubernetes Cluster\\n\\nThe following Terraform configuration sets up the Keycloak clients, user accounts, and group-based access control. Save it to a file called `keycloak.tf` and run `terraform init && terraform apply --auto-aprove`.\\n\\n<details>\\n<summary>Keycloak Terraform Configuration</summary>\\n\\nMake sure to update the highlighted values accordingly.\\n\\n```terraform\\nterraform {\\n  required_providers {\\n    keycloak = {\\n      source  = \\"keycloak/keycloak\\"\\n      version = \\">= 5.0.0\\"\\n    }\\n  }\\n}\\n\\nprovider \\"keycloak\\" {\\n  client_id = \\"admin-cli\\"\\n  username  = \\"admin\\"\\n  //highlight-start\\n  password  = \\"<YOUR ADMIN PASSWORD>\\"\\n  url       = \\"https://lemur-10.cloud-iam.com/auth\\" # double check the URL!\\n  realm     = \\"myrealm\\"\\n  //highlight-end\\n}\\n\\nlocals {\\n  //highlight-next-line\\n  realm_id = \\"myrealm\\"\\n  groups   = [\\"kube-dev\\", \\"kube-admin\\"]\\n  user_groups = {\\n    user-dev   = [\\"kube-dev\\"]\\n    backstage  = [\\"kube-dev\\"]\\n    user-admin = [\\"kube-admin\\"]\\n  }\\n}\\n\\nresource \\"keycloak_group\\" \\"groups\\" {\\n  for_each = toset(local.groups)\\n  realm_id = local.realm_id\\n  name     = each.key\\n}\\n\\nresource \\"keycloak_user\\" \\"users\\" {\\n  for_each       = local.user_groups\\n  realm_id       = local.realm_id\\n  username       = each.key\\n  enabled        = true\\n  email          = \\"${each.key}@kratix.io\\"\\n  email_verified = true\\n  first_name     = each.key\\n  last_name      = each.key\\n  initial_password {\\n    value = each.key\\n  }\\n}\\n\\nresource \\"keycloak_user_groups\\" \\"user_groups\\" {\\n  for_each  = local.user_groups\\n  realm_id  = local.realm_id\\n  user_id   = keycloak_user.users[each.key].id\\n  group_ids = [for g in each.value : keycloak_group.groups[g].id]\\n}\\n\\nresource \\"keycloak_openid_client_scope\\" \\"groups\\" {\\n  realm_id               = local.realm_id\\n  name                   = \\"groups\\"\\n  include_in_token_scope = true\\n  gui_order              = 1\\n}\\n\\nresource \\"keycloak_openid_group_membership_protocol_mapper\\" \\"groups\\" {\\n  realm_id        = local.realm_id\\n  client_scope_id = keycloak_openid_client_scope.groups.id\\n  name            = \\"groups\\"\\n  claim_name      = \\"groups\\"\\n  full_path       = false\\n}\\n\\nresource \\"keycloak_openid_client\\" \\"kube\\" {\\n  realm_id                     = local.realm_id\\n  client_id                    = \\"kube\\"\\n  name                         = \\"kube\\"\\n  enabled                      = true\\n  access_type                  = \\"CONFIDENTIAL\\"\\n  client_secret                = \\"kube-client-secret\\"\\n  standard_flow_enabled        = false\\n  implicit_flow_enabled        = false\\n  direct_access_grants_enabled = true\\n}\\n\\nresource \\"keycloak_openid_client\\" \\"backstage-client\\" {\\n  realm_id                     = local.realm_id\\n  client_id                    = \\"backstage-client\\"\\n  name                         = \\"backstage-client\\"\\n  enabled                      = true\\n  access_type                  = \\"CONFIDENTIAL\\"\\n  client_secret                = \\"backstage-client-secret\\"\\n  standard_flow_enabled        = true\\n  implicit_flow_enabled        = false\\n  direct_access_grants_enabled = true\\n  valid_redirect_uris = [\\n    \\"http://localhost:7007/api/auth/keycloak/handler/frame\\",\\n    \\"http://localhost:3003/api/auth/keycloak/handler/frame\\",\\n    \\"http://localhost:3000/api/auth/keycloak/handler/frame\\",\\n  ]\\n}\\n\\nresource \\"keycloak_openid_audience_protocol_mapper\\" \\"backstage_client_audience\\" {\\n  realm_id                 = local.realm_id\\n  client_id                = keycloak_openid_client.backstage-client.id\\n  name                     = \\"kube-audience\\"\\n  included_custom_audience = \\"kube\\"\\n}\\n\\nresource \\"keycloak_openid_client_default_scopes\\" \\"backstage-client\\" {\\n  realm_id  = local.realm_id\\n  client_id = keycloak_openid_client.backstage-client.id\\n  default_scopes = [\\n    \\"email\\", \\"basic\\", \\"profile\\", \\"groups\\"\\n  ]\\n}\\n\\nresource \\"keycloak_openid_client_default_scopes\\" \\"kube\\" {\\n  realm_id  = local.realm_id\\n  client_id = keycloak_openid_client.kube.id\\n  default_scopes = [\\n    \\"email\\",\\n    keycloak_openid_client_scope.groups.name,\\n  ]\\n}\\n```\\n\\n</details>\\n\\n## Deploying the Cluster\\n\\nNow that your Keycloak instance is up and running, and the resources are created, you can connect your Kubernetes cluster to it. In this blog post, we will deploy a KinD cluster. For clusters deployed by other means, please check the appropriate documentation.\\n\\n### Cluster configuration\\n\\nTo deploy a KinD cluster with OIDC, save the following configuration to a file called `config.yaml`:\\n\\n```yaml\\nkind: Cluster\\napiVersion: kind.x-k8s.io/v1alpha4\\nkubeadmConfigPatches:\\n  - |\\n    kind: ClusterConfiguration\\n    apiServer:\\n      extraArgs:\\n        oidc-client-id: kube\\n        # Update this!\\n        # highlight-next-line\\n        oidc-issuer-url:  https://lemur-10.cloud-iam.com/auth/realms/myrealm\\n        oidc-username-claim: email\\n        oidc-groups-claim: groups\\nnodes:\\n  - role: control-plane\\n    extraMounts:\\n    extraPortMappings:\\n      - containerPort: 31337\\n        hostPort: 31337\\n      - containerPort: 31340\\n        hostPort: 31340\\n      - containerPort: 31333\\n        hostPort: 31333\\n```\\n\\nYou can now use the Kratix quick-start script to deploy the Cluster with OIDC and install\\nKratix on it. If you don\'t have the Kratix repository cloned locally, run:\\n\\n```bash\\ngit clone https://github.com/syntasso/kratix.git\\ncd kratix\\n```\\n\\nNow, from the Kratix repository, run:\\n\\n```bash\\nKIND_PLATFORM_CONFIG=/path/to/config.yaml \\\\\\n  ./scripts/quick-start.sh \\\\\\n  --git-and-minio \\\\\\n  --recreate\\n```\\n\\n:::tip\\n\\nYou can also deploy the cluster directly with KinD:\\n\\n```bash\\nkind create cluster --config config.yaml --name platform\\n```\\n\\n:::\\n\\n### Creating the RBAC resources\\n\\nNext, you need to give the groups the necessary permissions to access the cluster. If you check the terraform configuration, you will see that the groups defined in the `user_groups` variable: `kube-dev` and `kube-admin`.\\n\\nCreate the necessary RBAC resources:\\n\\n- `kube-admin` should have `cluster-admin` role\\n- `kube-dev` should have be able to read every resource, but not create, update or delete\\n\\nThe ClusterRole and ClusterRoleBinding resources can be defined as follows:\\n\\n```yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: kube-admin\\nsubjects:\\n  - kind: Group\\n    name: kube-admin\\n    apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cluster-admin\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cr-reader\\nrules:\\n  - apiGroups:\\n      - \\"*\\"\\n    resources:\\n      - \\"*\\"\\n    verbs:\\n      - get\\n      - list\\n      - watch\\n---\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: kube-dev\\nsubjects:\\n  - kind: Group\\n    name: kube-dev\\n    apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cr-reader\\n```\\n\\nSave that to a file called `rbac.yaml` and apply it to your cluster:\\n\\n```bash\\nkubectl apply -f rbac.yaml --context kind-platform\\n```\\n\\n### Validating user access\\n\\nTo validate the user access, you will need to:\\n\\n- Login with both users.\\n- Check that the `user-admin` (that\'s part of the `kube-admin` group) user can create a resource.\\n- Check that the `user-dev` (that\'s part of the `kube-dev` group) user can read but not create resources.\\n\\nThe easiest way to login is to execute the script below. First, export the KEYCLOAK_URL env variable with the Keycloak issuer URL:\\n\\n```bash\\nexport KEYCLOAK_URL=\\"https://lemur-10.cloud-iam.com/auth/realms/myrealm\\"\\n```\\n\\nThen, run the script below:\\n\\n```bash\\nkubectl_config(){\\n  local ENDPOINT=${KEYCLOAK_URL}/protocol/openid-connect/token\\n  local ID_TOKEN=$(curl --silent -k -X POST $ENDPOINT \\\\\\n    -d grant_type=password \\\\\\n    -d client_id=kube \\\\\\n    -d client_secret=kube-client-secret \\\\\\n    -d username=$1 \\\\\\n    -d password=$1 \\\\\\n    -d scope=openid \\\\\\n    -d response_type=id_token | jq -r \'.id_token\')\\n  local REFRESH_TOKEN=$(curl --silent -k -X POST $ENDPOINT \\\\\\n    -d grant_type=password \\\\\\n    -d client_id=kube \\\\\\n    -d client_secret=kube-client-secret \\\\\\n    -d username=$1 \\\\\\n    -d password=$1 \\\\\\n    -d scope=openid \\\\\\n    -d response_type=id_token | jq -r \'.refresh_token\')\\n\\n  kubectl config set-credentials $1 \\\\\\n    --auth-provider=oidc \\\\\\n    --auth-provider-arg=client-id=kube \\\\\\n    --auth-provider-arg=client-secret=kube-client-secret \\\\\\n    --auth-provider-arg=idp-issuer-url=$KEYCLOAK_URL \\\\\\n    --auth-provider-arg=id-token=$ID_TOKEN \\\\\\n    --auth-provider-arg=refresh-token=$REFRESH_TOKEN\\n\\n  kubectl config set-context $1 --cluster=kind-platform --user=$1\\n}\\n\\n# setup config for our users\\nkubectl_config user-admin\\nkubectl_config user-dev\\n```\\n\\nThe script above should create two new contexts: `user-admin` and `user-dev`. You can easily switch between them by using the `--context` flag. To test the access the different users have, you can run the following:\\n\\n```bash\\nkubectl --context user-admin create namespace test-ns\\n# namespace/test-ns created\\n\\nkubectl --context user-dev create namespace test-ns-2\\n# Error from server (Forbidden): namespaces is forbidden: User \\"user-dev@syntasso.io\\" cannot create resource \\"namespaces\\" in API group \\"\\" at the cluster scope\\n\\nkubectl --context user-dev get namespace test-ns\\n# NAME      STATUS   AGE\\n# test-ns   Active   1m\\n\\nkubectl --context user-dev delete namespace test-ns\\n# Error from server (Forbidden): namespaces \\"test-ns\\" is forbidden: User \\"user-dev@syntasso.io\\" cannot delete resource \\"namespaces\\" in API group \\"\\" in the namespace \\"test-ns\\"\\n\\nkubectl --context user-admin delete namespace test-ns\\n# namespace \\"test-ns\\" deleted\\n```\\n\\nGreat! You can now move on configuring Backstage.\\n\\n## Configuring Backstage\\n\\nAssuming you already have a Backstage instance, you will need to do the following changes to configure it to use Keycloak for authentication:\\n\\n- Create an API reference to identify the Keycloak provider.\\n- Create the API factory that will handle the authentication.\\n- Create the Keycloak Auth provider Resolver.\\n- Configure the provider to access the Keycloak instance.\\n- Add Keycloak to the sign in page so users can login with it.\\n\\nThe steps above are explained in details as part of the [OIDC provider from scratch](https://backstage.io/docs/auth/oidc) guide in the Backstage docs. Below, you will find the code snippets you will need to include in your Backstage instance.\\n\\n:::tip\\n\\nYou can skip this entire section by cloning the Backstage app available [here](https://github.com/syntasso/blog-backstage-oidc). You still need to authenticate with the Syntasso Registry to run `yarn install`. Check [Accessing the Private NPM registry](/ske/integrations/backstage/plugins#local-development) docs for details.\\n\\nCheck the README in the repository for details about how to run the app.\\n\\n:::\\n\\n### The API reference\\n\\nOpen the `packages/app/src/apis.ts` file and add the following snippet:\\n\\n```js\\nimport {\\n  AnyApiFactory,\\n  configApiRef,\\n  createApiFactory,\\n  OpenIdConnectApi,\\n  ProfileInfoApi,\\n  BackstageIdentityApi,\\n  SessionApi,\\n  createApiRef,\\n  ApiRef,\\n} from \\"@backstage/core-plugin-api\\";\\n\\nexport const keycloakOIDCAuthApiRef: ApiRef<\\n  OpenIdConnectApi & ProfileInfoApi & BackstageIdentityApi & SessionApi\\n> = createApiRef({\\n  id: \\"auth.keycloak\\",\\n});\\n```\\n\\n### The API factory\\n\\nStill in the `apis.ts` file, create a new API factory and add to the `apis` array:\\n\\n```js\\nimport { OAuth2 } from \\"@backstage/core-app-api\\";\\nimport {\\n  discoveryApiRef,\\n  oauthRequestApiRef,\\n} from \\"@backstage/core-plugin-api\\";\\n\\nexport const apis: AnyApiFactory[] = [\\n  /* this is the new API factory */\\n  createApiFactory({\\n    api: keycloakOIDCAuthApiRef,\\n    deps: {\\n      discoveryApi: discoveryApiRef,\\n      oauthRequestApi: oauthRequestApiRef,\\n      configApi: configApiRef,\\n    },\\n    factory: ({ discoveryApi, oauthRequestApi, configApi }) =>\\n      OAuth2.create({\\n        configApi,\\n        discoveryApi,\\n        oauthRequestApi,\\n        provider: {\\n          id: \\"keycloak\\",\\n          title: \\"Keycloak Provider\\",\\n          icon: () => null,\\n        },\\n        environment: configApi.getOptionalString(\\"auth.environment\\"),\\n        defaultScopes: [\\"openid\\", \\"profile\\", \\"email\\"],\\n        popupOptions: {\\n          size: { fullscreen: true },\\n        },\\n      }),\\n  }),\\n\\n  /* these are the existing API factories */\\n  createApiFactory({\\n    api: scmIntegrationsApiRef,\\n    deps: { configApi: configApiRef },\\n    factory: ({ configApi }) => ScmIntegrationsApi.fromConfig(configApi),\\n  }),\\n  ScmAuth.createDefaultApiFactory(),\\n];\\n```\\n\\n### The Keycloak Auth Resolver\\n\\nYou will need to include a couple of modules to your backend dependencies:\\n\\n```bash\\nyarn --cwd packages/backend add @backstage/plugin-auth-backend-module-oidc-provider\\nyarn --cwd packages/backend add @backstage/backend-plugin-api\\nyarn --cwd packages/backend add @backstage/catalog-model\\n```\\n\\nNext, open the `packages/backend/src/index.js` and add register the keycloak provider:\\n\\n:::tip\\n\\nMake sure to include the snippet below _before_ the `backend.start()` call.\\n\\n:::\\n\\n```js\\n\\nimport {\\n  authProvidersExtensionPoint,\\n  createOAuthProviderFactory,\\n  OAuthAuthenticator,\\n} from \'@backstage/plugin-auth-node\';\\nimport { oidcAuthenticator } from \'@backstage/plugin-auth-backend-module-oidc-provider\';\\nimport { createBackendModule } from \'@backstage/backend-plugin-api\';\\nimport {\\n  stringifyEntityRef,\\n  DEFAULT_NAMESPACE,\\n} from \'@backstage/catalog-model\';\\n\\nconst kcAuthProviderModule = createBackendModule({\\n  // This ID must be exactly \\"auth\\" because that\'s the plugin it targets\\n  pluginId: \'auth\',\\n  // This ID must be unique, but can be anything\\n  moduleId: \'keycloak\',\\n  register(reg) {\\n    reg.registerInit({\\n      deps: { providers: authProvidersExtensionPoint },\\n      async init({ providers }) {\\n        providers.registerProvider({\\n          // This ID must match the actual provider config, e.g. addressing\\n          // auth.providers.azure means that this must be \\"azur e\\".\\n          providerId: \'keycloak\',\\n          // Use createProxyAuthProviderFactory instead if it\'s one of the proxy\\n          // based providers rather than an OAuth based one\\n          factory: createOAuthProviderFactory({\\n            // For more info about authenticators please see https://backstage.io/docs/auth/add-auth-provider/#adding-an-oauth-based-provider\\n            authenticator: oidcAuthenticator as OAuthAuthenticator<unknown, any>,\\n            async signInResolver(info, ctx) {\\n              console.log(info);\\n              const userRef = stringifyEntityRef({\\n                kind: \'User\',\\n                name: info?.result.fullProfile.userinfo.name as string,\\n                namespace: DEFAULT_NAMESPACE,\\n              });\\n              return ctx.issueToken({\\n                claims: {\\n                  sub: userRef, // The user\'s own identity\\n                  ent: [userRef], // A list of identities that the user claims ownership through\\n                  tok: info?.result.session.accessToken, // this is important!\\n                },\\n              });\\n            },\\n          }),\\n        });\\n      },\\n    });\\n  },\\n});\\n\\nbackend.add(kcAuthProviderModule);\\n```\\n\\n### The provider configuration\\n\\nIn your `app-config.yaml`, add the following configuration, making sure you update the URLs accordingly:\\n\\n```yaml\\nauth:\\n  environment: development\\n  session:\\n    secret: a-session-secret\\n  providers:\\n    keycloak:\\n      development:\\n        metadataUrl: ${KEYCLOAK_URL}/.well-known/openid-configuration\\n        clientId: backstage-client\\n        clientSecret: backstage-client-secret\\n        prompt: auto\\n        authorizationUrl: ${KEYCLOAK_URL}/protocol/openid-connect/auth\\n        tokenUrl: ${KEYCLOAK_URL}/protocol/openid-connect/token\\n```\\n\\n### The Sign In page\\n\\nFinally, add Keycloak to the SignInPage component in the `packages/app/src/App.tsx` file:\\n\\n```js\\nimport { keycloakOIDCAuthApiRef } from \\"./apis\\";\\n\\nconst app = createApp({\\n  apis,\\n  /* ... */\\n  components: {\\n    SignInPage: (props) => (\\n      <SignInPage\\n        {...props}\\n        auto\\n        providers={[\\n          \\"guest\\",\\n          {\\n            id: \\"keycloak\\",\\n            title: \\"Keycloak\\",\\n            message: \\"Sign in using Keycloak\\",\\n            apiRef: keycloakOIDCAuthApiRef,\\n          },\\n        ]}\\n      />\\n    ),\\n  },\\n});\\n```\\n\\n### Testing it all together\\n\\nAt this stage, you should be able to sign in with Keycloak and access your Backstage instance.\\n\\nIn a terminal, start your Backstage app and open it in your browser.\\n\\n:::tip\\n\\nDepending on your version of Backstage, you may need to run `yarn dev` or `yarn start` to start the app.\\n\\n:::\\n\\nYou should see the Sign In page with Keycloak as an option:\\n\\n```mdx-code-block\\nimport figure01 from \\"./figure01.png\\"\\nimport figure02 from \\"./figure02.png\\"\\nimport figure03 from \\"./figure03.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure01} alt=\\"The Backstage Sign In page showing both the Guest and the Keycloak Authentication options\\" />\\n\\n  <figcaption>Sign In Page</figcaption>\\n</figure>\\n\\nClicking in the Keycloak option will redirect you to the Keycloak login page:\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure02} alt=\\"The Keycloak login page. Username and password fields are populated with user-admin\\" />\\n\\n  <figcaption>The Keycloak login page</figcaption>\\n</figure>\\n\\nAfter logging in, you will be redirected back to Backstage and you should be logged in. If you navigate to the Settings page, you should see your user information:\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure03} alt=\\"The Settings Page on Backstage, showing the logged in user information\\" />\\n\\n  <figcaption>The Settings page</figcaption>\\n</figure>\\n\\nExcellent! Final step is to configure the SKE plugins to communicate with your Platform cluster, using the Keycloak token.\\n\\n## Configuring the SKE plugins\\n\\nThis section assumes you have a SKE License token, have access to the SKE Backstage plugins, and installed them in your Backstage instance (or that you have cloned the [example Backstage app](https://github.com/syntasso/blog-backstage-oidc)). If that\'s not your case,you can follow a step-by-step guide on how to configure the SKE Backstage plugins [here](/ske/integrations/backstage/plugins).\\n\\n:::tip\\n\\nMake sure you have the latest version of the SKE plugins installed. This post requires, at a minimum, ske-backend v0.14.0 and ske-frontend v0.13.0.\\n\\n:::\\n\\nThe configuration of the plugins is very straightforward. All you need to do is:\\n\\n- Add the Platform cluster to your app config.\\n- Tell the SKE Plugins the name of the platform cluster.\\n\\nBoth of those changes are done via the `app-config.yaml` file.\\n\\nLocate the `kubernetes` section (or create one if it doesn\'t exist) and add the Platform cluster:\\n\\n```yaml\\nkubernetes:\\n  serviceLocatorMethod:\\n    type: multiTenant\\n  clusterLocatorMethods:\\n    - type: config\\n      clusters:\\n        - url: \\"https://<your kind cluster url\\"\\n          name: kratix-platform\\n          authProvider: oidc\\n          oidcTokenProvider: keycloak\\n          skipTLSVerify: true\\n```\\n\\n:::tip\\n\\nTo find your KinD cluster URL, run `kubectl cluster-info --context kind-platform`\\n\\n:::\\n\\nYou must now tell the backend plugin which cluster is the platform cluster. For that, add the following section to your `app-config.yaml`:\\n\\n```yaml\\nske:\\n  kubernetes:\\n    # make sure this matches name of the cluster in the `kubernetes` section\\n    platformName: kratix-platform\\n```\\n\\nExcellent. You should now have a working Backstage instance with Keycloak authentication and SKE plugins configured to work with your Platform cluster. Let\'s test it all together now.\\n\\n## Testing it all together\\n\\n### Populating the Catalog\\n\\nIf you have SKE and Backstage fully configured, you could try to install a Promise and create a new resource via the Template. If not, you can register the template available [here](https://github.com/syntasso/blog-backstage-oidc/blob/main/examples/ske-entities.yaml) directly in your instance.\\n\\n:::tip\\n\\nIf you cloned the example backstage application, you can skip this step.\\n\\n:::\\n\\nFor that:\\n\\n1. Open your `app-config.yaml`\\n1. Locate the `catalog.locations` section\\n1. Add to the list:\\n  ```yaml\\n  - type: url\\n    target: https://github.com/syntasso/blog-backstage-oidc/blob/main/examples/ske-entities.yaml\\n    rules:\\n      - allow: [Template]\\n  ```\\n\\nNow you restart your Backstage instance.\\n\\n### Using the template\\n\\nOnce you have a template registered in your Backstage instance, you can use it to create a new resource. This could either be your the Template generated by your Promise or the template you registered in the previous step. We will assume you are using the example Template in the next steps.\\n\\nLocate and click the CREATE button in the sidebar and select the ConfigMap:\\n\\n```mdx-code-block\\nimport figure04 from \\"./figure04.png\\"\\nimport figure05 from \\"./figure05.png\\"\\nimport figure06 from \\"./figure06.png\\"\\nimport figure07 from \\"./figure07.png\\"\\nimport figure08 from \\"./figure08.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure04} alt=\\"The Create page\\" />\\n\\n  <figcaption>The Create Page</figcaption>\\n</figure>\\n\\nFollow the steps and configure a new ConfigMap. Don\'t forget to click the `+` icon to add key-value pairs to your ConfigMap.\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure05} alt=\\"The form to create a configmap\\" />\\n\\n  <figcaption>The ConfigMap Template</figcaption>\\n</figure>\\n\\nProceed with the form and create the ConfigMap:\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure06} alt=\\"The page backstage shows once you create the configmap\\" />\\n\\n  <figcaption>The ConfigMap created</figcaption>\\n</figure>\\n\\nAt this stage, you should have a new ConfigMap in your Cluster:\\n\\n```bash\\n$ kubectl --context user-admin get configmap test-configmap # or the name you used!\\nNAME             DATA   AGE\\ntest-configmap   2      1m\\n```\\n\\nGreat! Now logout and login with the `user-dev` user. To logout, click `Settings` on the bottom-left corner, then the three dots, and Sign Out\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure07} alt=\\"The sign out process\\" />\\n\\n  <figcaption>Signing out page</figcaption>\\n</figure>\\n\\nLogged in as the `user-dev` user, try to create a new ConfigMap following the same steps.\\n\\nYou should get a 403 error, since the `user-dev` user does not have the required permissions:\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={figure08} alt=\\"A Screenshot of the Backstage UI with a 403 error for the user-dev\\" />\\n\\n  <figcaption>Creating the ConfigMap failed</figcaption>\\n</figure>\\n\\n\ud83c\udf89\\n\\n## Conclusion\\n\\nBy integrating Keycloak with both Kubernetes and Backstage, and configuring the SKE Backstage plugins to leverage OIDC tokens, we\'ve created a seamless and secure developer experience. This setup not only simplifies authentication and authorisation but also enables fine-grained access control across your platform, allowing you to manage who-can-do-what directly from your Kubernetes cluster.\\n\\nCommunicating directly to the Platform cluster opens up a new world of possibilities of what Kratix can now surface to Backstage. Most up-to-date status, pipeline progress, logs, and much more. We are still prioritising what we think will bring the most value, but we are very curious to hear any feedback or suggestion you might have.\\n\\nWhile article focuses on the Backstage, Kratix can be integrated with any portal or interface. For example, check out [this video](https://www.youtube.com/watch?v=7nKx4CnEvoY) to see how Kratix can integrate with [Port](https://port.io). We\'re also keen to hear what you are using (or planning to use) as the interface to your platform in your organisation, and hearing how could Kratix help you out.\\n\\nAs always, ping us directly via [Github](https://github.com/syntasso/kratix) or in our [Community Slack](https://kratixworkspace.slack.com/)."},{"id":"debugging-promise-updates","metadata":{"permalink":"/blog/debugging-promise-updates","source":"@site/blog/2025-03-05-debugging/index.mdx","title":"Debugging in Kratix","description":"Identifying the tools you can use to debug your Promise rollouts","date":"2025-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"kratix internals","permalink":"/blog/tags/kratix-internals"},{"inline":true,"label":"debugging","permalink":"/blog/tags/debugging"}],"readingTime":11.69,"hasTruncateMarker":true,"authors":[{"name":"Sapphire Mason-Brown","title":"Engineer @ Syntasso","url":"https://github.com/saphmb","imageURL":"https://github.com/saphmb.png","key":"sapphire","page":null}],"frontMatter":{"slug":"debugging-promise-updates","title":"Debugging in Kratix","description":"Identifying the tools you can use to debug your Promise rollouts","authors":["sapphire"],"tags":["kratix","kratix internals","debugging"]},"unlisted":false,"prevItem":{"title":"Kratix, Backstage, and OIDC","permalink":"/blog/backstage-and-keycloak"},"nextItem":{"title":"How to write Compound Promises","permalink":"/blog/compound-promises"}},"content":"As much as we would all like, rolling out updates to any software can result in some bumps along the way. This applies to updates to Promises too but Kratix has some feature to help identify any issues within your Promise spec, your Promise workflows and the scheduling of documents outputted by your workflows.\\n\\nIn this blog post we\'ll explore some of the common issues that users experience when configuring Kratix and developing Promises and well as how Kratix tries to steer you in the right direction when something goes wrong. We\'ll be exploring:\\n\\n- Querying Kratix effectively with labels\\n- Debugging scheduling issues Kratix\\n- Getting information from Destination and State Store status updates\\n- Validating the Kratix Promise spec\\n\\nClick on \\"read more\\" to continue!\\n\\n{/* truncate */}\\n\\n## The end goal\\n\\nWe\'ll be working with the Runtime Promise which deploys a Deployment configured with Nginx. By making updates to the Promise and Kratix resources, we\'ll highlight some common problems and the breadcrumbs you can follow to solve them.\\n\\nYou can follow the steps in this post and debug in your own environment. If you want to do this, start with the Runtime Promise [here](https://github.com/syntasso/kratix-docs/tree/main/assets/runtime-promise).\\n\\nA central property someone making a request of this Promise needs to provide is the image for their Deployment. To optimise this Promise, we want to add a new Pipeline step that performs a security scan of the provided image and outputs the result as a HealthRecord in Kratix. To get started, we\'ll deploy the Promise to our testing environment.\\n\\n## Getting Started\\n\\nThe first thing we want to do  is set up a kratix environment, to get started quickly we\'ll be deploying Kratix on Kind clusters via some helper scripts in the Kratix repo. If you\'re playing along, clone the Kratix repository and run:\\n\\n```\\nmake quick-start\\nmake prepare-platform-as-destination\\n```\\n\\n## Debugging Scheduling in Kratix\\n\\nIf you\'re playing along, you can clone the Runtime Promise by running:\\n\\n```\\ngit clone --depth=1 https://github.com/syntasso/kratix-docs.git runtime-promise\\ncd runtime-promise\\ngit sparse-checkout set assets/runtime-promise --no-cone\\n```\\n\\nLet\'s install the Promise with:\\n\\n```bash\\nkubectl apply -f promise.yaml\\n```\\n\\nThe runtime promise allows users to deploy an Application Runtime as a service via a Resource Request where they can edit the `lifecycle`, `image`, `servicePort` and the number of `replicas` in their deployment.\\n\\nThe `lifecycle` field determines which Destination to schedule the workloads to and this maps to the label `environment=${lifecycle}` on the Destinations. Whilst working on the Promise, we want to deploy it to a `testing` Destination so the request will look as follows:\\n\\n```yaml\\napiVersion: marketplace.kratix.io/v1alpha1\\nkind: Runtime\\nmetadata:\\n  name: example-runtime\\n  namespace: default\\nspec:\\n  lifecycle: testing\\n  image: syntasso/website\\n  servicePort: 80\\n  replicas: 1\\n```\\n\\nCreate a file `example-runtime.yaml` with these contents and apply this request with `kubectl apply -f example-runtime.yaml`.\\n\\nWe can query for the pods created as part of the workflow with selectors that are added to workflow pods by default, this is particularly useful in busy environments with a lot of running pods:\\n\\n```bash\\nkubectl get pods --selector kratix.io/promise-name=runtime,\\\\\\nkratix.io/workflow-type=resource,\\\\\\nkratix.io/workflow-action=configure,\\\\\\nkratix.io/resource-name=example-runtime\\n```\\n\\nThe output should look something like this:\\n\\n```bash\\nNAME                                                  READY   STATUS      RESTARTS   AGE\\nkratix-runtime-example-runtime-instance-24bcb-ffh7w   0/1     Completed   0          20m\\n```\\n\\nAs the workflow has finished running, we can check to ensure the documents were scheduled to the `testing` Destination. Like workflow pods, Works are created with a set of default labels to make querying for Works associated with given Promises and Resource Requests easier. Run the following to get the Work associated with the `example-runtime` resource:\\n\\n```bash\\nkubectl get work --selector kratix.io/resource-name=example-runtime -o yaml\\n```\\n\\nThe status of the Work shows that it has not been scheduled:\\n\\n```\\nStatus:\\n  Conditions:\\n  - lastTransitionTime: \\"2025-03-06T07:00:45Z\\"\\n    message: \'No Destinations available work WorkloadGroups: [ae2b1fca515949e5d54fb22b8ed95575]\'\\n    reason: UnscheduledWorkloadGroups\\n    status: \\"False\\"\\n    type: Scheduled\\n  - lastTransitionTime: \\"2025-03-06T07:00:45Z\\"\\n    message: WorkGroups that have been scheduled are at the correct Destination(s)\\n    reason: ScheduledToCorrectDestinations\\n    status: \\"False\\"\\n    type: Misscheduled\\n```\\n\\nWhat does this mean? Essentially, there were no Destinations matching the label `environment=testing` in our environment. Lets review the available Destinations and their labels with:\\n\\n```\\nkubectl get destinations --show-labels\\n```\\n\\nThis produces:\\n\\n```bash\\nNAME       READY   LABELS\\nplatform   True    environment=platform\\nworker-1   True    environment=dev\\n```\\n\\nThere _is_ no destination with the `environment=testing` label and as a result, the documents could not be scheduled. Lets create the `testing` Destination. To do this we will:\\n1. Create a new Cluster\\n2. Create a backing State Store for the cluster\\n3. Create a new Destination\\n\\nAs we are running on kind, we can can create a new cluster by running:\\n\\n```\\nkind create cluster --image kindest/node:v1.31.2 --name worker-2\\nexport WORKER_2=\\"kind-worker-2\\"\\n```\\n\\nNext, we need to ensure GitOps tooling is available on the new cluster. This can be quickly aided by the Kratix repo again, from the root of the repo, run the following:\\n\\n```bash\\n./scripts/install-gitops --context ${WORKER_2} --path worker-2\\n```\\n\\nOur quick start has configured minio on the cluster so we can use the minio endpoint within the BucketStateStore. Run the following to create the BucketStateStore:\\n\\n```bash\\ncat <<EOF > testing-bucket.yaml\\napiVersion: platform.kratix.io/v1alpha1\\nkind: BucketStateStore\\nmetadata:\\n  name: testing\\nspec:\\n  authMethod: accessKey\\n  bucketName: kratix\\n  endpoint: minio.kratix-platform-system.svc.cluster.local\\n  insecure: true\\n  secretRef:\\n    name: minio\\n    namespace: default\\nstatus: {}\\nEOF\\n\\nkubectl apply -f testing-bucket.yaml --context kind-platform\\n```\\n\\nNow we can create the Destination that is backed by this State Store:\\n\\n```yaml\\ncat <<EOF > testing-destination.yaml\\napiVersion: platform.kratix.io/v1alpha1\\nkind: Destination\\nmetadata:\\n  labels:\\n    environment: testing\\n  name: testing\\nspec:\\n  cleanup: none\\n  filepath:\\n    mode: nestedByMetadata\\n  stateStoreRef:\\n    kind: BucketStateStore\\n    name: testing\\nstatus: {}\\nEOF\\n\\nkubectl apply -f testing-destination.yaml --context kind-platform\\n```\\n\\nAfter applying both of these we should set that we have a new running BucketStateStore and Destination. However, when running the following:\\n\\n```bash\\nkubectl get destinations.platform.kratix.io testing --context kind-platform\\n```\\n\\nwe observe that the `testing` Destination is not Ready\\n\\n```bash\\nNAME      READY\\ntesting   False\\n```\\n\\nSimilarly, when querying the BucketStateStore with:\\n\\n```bash\\nkubectl get BucketStateStore testing --context kind-platform\\n```\\n\\nwe can also see that the State Store is not Ready:\\n\\n```bash\\nNAME      READY\\ntesting   False\\n```\\n\\nWhy is this the case? Lets `kubectl describe` the `testing` Destination. Run:\\n\\n```bash\\nkubectl describe destination testing --context kind-platform\\n```\\n\\nThe `status` of the Destination includes some conditions which detail why it is not yet ready:\\n\\n```yaml\\nStatus:\\n  Conditions:\\n    Last Transition Time:  2025-03-05T11:56:05Z\\n    Message:               Unable to write test documents to State Store\\n    Reason:                StateStoreWriteFailed\\n    Status:                False\\n    Type:                  Ready\\n```\\n\\nThis is reiterated by an event that was fired:\\n\\n```\\nEvents:\\n  Type     Reason               Age   From                   Message\\n  ----     ------               ----  ----                   -------\\n  Warning  DestinationNotReady  20m   DestinationController  Failed to write test documents to Destination \\"testing\\": secret \\"minio\\" not found in namespace \\"default\\"\\n```\\n\\nWhen creating both Destinations and State Stores, Kratix checks to see that the defined locations can be written to with the provided credentials before marking them as `Ready`. We see a similar `status` and `event` fired for the `testing` BucketStateStore:\\n\\n```bash\\nStatus:\\n  Conditions:\\n    Last Transition Time:  2025-03-05T11:30:07Z\\n    Message:               Error initialising writer: secret \\"minio\\" not found in namespace \\"default\\"\\n    Reason:                ErrorInitialisingWriter\\n    Status:                False\\n    Type:                  Ready\\n  Status:                  NotReady\\nEvents:\\n  Type     Reason    Age   From                        Message\\n  ----     ------    ----  ----                        -------\\n  Warning  NotReady  52m   BucketStateStoreController  BucketStateStore \\"testing\\" is not ready: Error initialising writer: secret \\"minio\\" not found in namespace \\"default\\"\\n```\\n\\nThis means that no work can be scheduled to these destinations until the problems are remedied, so lets fix the issue. Our minio credential isn\'t quite right, we need to edit the `testing` BucketStateStore to update the name of the secretRef from `minio` to `minio-credentials`. Update the BucketStateStore and in just a few moments, both the State Store and Destination will become `Ready`\\n\\n```bash\\nStatus:\\n  Conditions:\\n    Last Transition Time:  2025-03-05T12:28:31Z\\n    Message:               State store is ready\\n    Reason:                StateStoreReady\\n    Status:                True\\n    Type:                  Ready\\n  Status:                  Ready\\nEvents:\\n  Type     Reason    Age   From                        Message\\n  ----     ------    ----  ----                        -------\\n  Warning  NotReady  59m   BucketStateStoreController  BucketStateStore \\"testing\\" is not ready: Error initialising writer: secret \\"minio\\" not found in namespace \\"default\\"\\n  Normal   Ready     65s   BucketStateStoreController  BucketStateStore \\"testing\\" is ready\\n```\\n\\n```bash\\nStatus:\\n  Conditions:\\n    Last Transition Time:  2025-03-05T12:28:31Z\\n    Message:               Test documents written to State Store\\n    Reason:                TestDocumentsWritten\\n    Status:                True\\n    Type:                  Ready\\nEvents:\\n  Type     Reason               Age   From                   Message\\n  ----     ------               ----  ----                   -------\\n  Warning  DestinationNotReady  34m   DestinationController  Failed to write test documents to Destination \\"testing\\": secret \\"minio\\" not found in namespace \\"default\\"\\n  Normal   Ready                114s  DestinationController  Destination \\"testing\\" is ready\\n```\\n\\nNow that the Destination is up and Healthy, we can see that the work has been scheduled successfully:\\n\\n```\\nstatus:\\n  conditions:\\n  - lastTransitionTime: \\"2025-03-06T07:08:03Z\\"\\n    message: All WorkloadGroups scheduled to Destination(s)\\n    reason: ScheduledToDestinations\\n    status: \\"True\\"\\n    type: Scheduled\\n```\\n\\nAnd, more importantly, our `example-runtime` app is up and running. We can visit it at http://example-runtime.default.local.gd:31338\\n\\n```mdx-code-block\\nimport app from \\"./app.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={app} alt=\\"An image of the running Runtime App in the browser\\" />\\n\\n  <figcaption>Runtime App</figcaption>\\n</figure>\\n\\nNow that it\'s deployed successfully, we\'re ready to build on the Runtime Promise and add the security scan as a new step when configuring resource requests.\\n\\nWe can bootstrap this step with the [Kratix CLI\'s](/main/kratix-cli/intro) `add container` command. From the root of the Runtime Promise directory, you can run:\\n\\n```bash\\nkratix add container resource/configure/instance \\\\\\n  --image ghcr.io/syntasso/kratix-docs/trivy-scan:v1.0.0 \\\\\\n  --name security-scan\\n```\\n\\nThis command adds a new container to the existing resource configure workflow with the name `security-scan` and the image `ghcr.io/syntasso/kratix-docs/trivy-scan:v1.0.0`.\\n\\nYour directory structure should now look like this:\\n\\n```bash\\n\u251c\u2500\u2500 example-resource.yaml\\n\u251c\u2500\u2500 promise.yaml\\n\u2514\u2500\u2500 workflows\\n    \u2514\u2500\u2500 resource\\n        \u2514\u2500\u2500 configure\\n            \u2514\u2500\u2500 instance\\n                \u251c\u2500\u2500 deploy-resources\\n                \u2502\xa0\xa0 \u251c\u2500\u2500 Dockerfile\\n                \u2502\xa0\xa0 \u251c\u2500\u2500 resources\\n                \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 postgres-request-template.yaml\\n                \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 redis-request-template.yaml\\n                \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 runtime-request-template.yaml\\n                \u2502\xa0\xa0 \u2514\u2500\u2500 scripts\\n                \u2502\xa0\xa0     \u2514\u2500\u2500 pipeline.rb\\n                \u2514\u2500\u2500 security-scan\\n                    \u251c\u2500\u2500 Dockerfile\\n                    \u251c\u2500\u2500 resources\\n                    \u2514\u2500\u2500 scripts\\n                        \u2514\u2500\u2500 pipeline.sh\\n```\\n\\nYou\'ll also see an addition to the `promise.yaml`, appending the `security-scan` container to the list of containers in the `resource-configure` Pipeline\\n\\n```yaml\\n  workflows:\\n    resource:\\n      configure:\\n        - apiVersion: platform.kratix.io/v1alpha1\\n          kind: Pipeline\\n          metadata:\\n            name: resource-configure\\n          spec:\\n            containers:\\n              - resource-configure\\n              image: ghcr.io/syntasso/kratix-docs/runtime-configure-pipeline:v0.1.0\\n              name: resource-configure\\n  #highlight-start\\n              - image: ghcr.io/syntasso/kratix-docs/trivy-scan:v1.0.0\\n                name: security-scan\\n  #highlight-end\\n```\\n\\nNext, we need to bring this image into existence. Update the new `pipeline.sh` file in the `security-scan` directory to look like this:\\n\\n```bash\\n#!/usr/bin/env sh\\n\\nset -euxo pipefail\\n\\nimage=\\"$(yq eval \'.spec.image\' /kratix/input/object.yaml)\\"\\n\\necho \\"Scanning ${image}\\"\\n\\nif [ $DEBUG = \\"true\\" ]; then\\n  DEBUG_MODE=true\\n  echo \\"Running in debug mode\\"\\nelse \\n  DEBUG_MODE=false\\nfi\\n\\nTRIVY_DEBUG=$DEBUG_MODE trivy image --format=json --output=results.json \\"${image}\\" > results.json\\n\\nhealth_state=\\"healthy\\"\\n\\nif [ \\"$(jq \'.[] | select(.Vulnerabilities != null) | length\' results.json)\\" != \\"\\" ]; then\\n  health_state=\\"degraded\\"\\nfi\\n\\nresource_name=$(yq \'.metadata.name\' /kratix/input/object.yaml)\\nnamespace=\\"default\\"\\n\\nmkdir -p /kratix/output/platform/\\n\\ncat <<EOF > /kratix/output/platform/health-record.yaml\\napiVersion: platform.kratix.io/v1alpha1\\nkind: HealthRecord\\nmetadata:\\n  name: rubyapp-${resource_name}\\n  namespace: ${namespace}\\ndata:\\n  promiseRef:\\n    name: rubyapp\\n  resourceRef:\\n    name: ${resource_name}\\n    namespace: ${namespace}\\n  state: ${health_state}\\n  lastRun: $(date +%s)\\n  details:\\n    results: \\"\\"\\nEOF\\n\\ncat results.json | yq -P > results.yaml\\nyq e -i \'.data.details.results = load(\\"results.yaml\\")\' /kratix/output/platform/health-record.yaml\\n\\ncat <<EOF > /kratix/metadata/destination-selectors.yaml\\n- directory: platform\\n  matchLabels:\\n    environment: platform\\nEOF\\n```\\n\\nThis script retrieves the `image` specified in the request, scans it with `trivy` and outputs a `HealthRecord` detailing the results.\\n\\nTo use Trivy, we also need to update the generated Dockerfile to install the Trivy CLI. Update your Dockerfile to look like this:\\n\\n```dockerfile\\nFROM \\"alpine\\"\\n\\nRUN apk update && apk add --no-cache yq curl jq\\n\\nRUN curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.18.3\\n\\nADD scripts/pipeline.sh /usr/bin/pipeline.sh\\nADD resources resources\\n\\nRUN chmod +x /usr/bin/pipeline.sh\\n\\nCMD [ \\"sh\\", \\"-c\\", \\"pipeline.sh\\" ]\\nENTRYPOINT []\\n```\\n\\nTo ensure the new security-scan image is are available on the kind clusters, we need to build it and load it onto the kind node. Run the following from the `runtime-promise/workflows/resource/configure/instance/security-scan` directory:\\n\\n```bash\\ndocker build . --tag ghcr.io/syntasso/kratix-docs/trivy-scan:v1.0.0\\nkind load docker-image ghcr.io/syntasso/kratix-docs/trivy-scan:v1.0.0 --name platform\\n```\\n\\nWhilst testing this command, we\'ll run in debug mode should any issues come up. Our script has already been configured to allow this to be set via an environment variable.\\n\\nAdd the following lines to the Promise spec for the newly introduced container:\\n\\n```yaml\\n- image: ghcr.io/syntasso/kratix-docs/trivy-scan:v1.0.0\\n  name: security-scan\\n  #highlight-start\\n  envs:\\n  - name: DEBUG\\n    value: \\"true\\"\\n  #highlight-end\\n```\\n\\nWe\'re nearly ready to install the Promise!\\n\\n## Identifying invalid workflows\\n\\nBefore you can install your Promise, Kratix ensures that your Promise has valid Workflows definitions that can be used to generate your workflow pods. Apply your updated promise with:\\n\\n```bash\\nkubectl apply -f promise.yaml\\n```\\n\\nYou should see a message that includes\\n\\n```bash\\njson: unknown field \\"envs\\"\\n```\\n\\nWe have a slight typo in the Promise spec for the new workflow, `envs` should be `env`. Correct this and apply the promise again.\\n\\nRe-applying the Promise will trigger the Workflows for the `example-runtime` request which should now generate a HealthRecord with the security scan results. Run:\\n\\n```\\nkubectl get healthrecord --context kind-platform\\n```\\n\\nand you should see output similar to:\\n\\n```\\nNAME                      STATUS    AGE\\nruntime-example-runtime   degraded  8m12s\\n```\\n\\nAlso, as the healthrecord reference the `example-runtime` request, the request should be updated to reflect the results in the HealthRecord. Run:\\n\\n```\\nkubectl describe runtime example-runtime --context kind-platform\\n```\\n\\nAnd the should see something similar to the following:\\n\\n```\\n  Health Record:\\n    Details:\\n      Results:\\n        ...\\n    Last Run:                               1741248436\\n    State:                                  degraded\\n```\\n\\nGreat! The updated Runtime Promise is now running a provisioning requests that \\n\\n## Overview\\n\\nWe\'ve explored some of the common stumbling blocks that can come up when working wih Kratix - issues with scheduling, configuring Destinations - and the features of Kratix you can use to as debugging tools.\\n\\nMany of these are new features we\'ve introduced following feedback from our customers so if there is a gotcha that has caught you out in the past, let us know via [Github](https://github.com/syntasso/kratix) or our [Community Slack](https://kratixworkspace.slack.com/) - we\'re alway happy to hear from users."},{"id":"compound-promises","metadata":{"permalink":"/blog/compound-promises","source":"@site/blog/2025-02-13-compound-promises/index.mdx","title":"How to write Compound Promises","description":"Everything you need to know about building Compound Promises effectively","date":"2025-02-13T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"promises","permalink":"/blog/tags/promises"},{"inline":true,"label":"compound promise","permalink":"/blog/tags/compound-promise"}],"readingTime":19.09,"hasTruncateMarker":true,"authors":[{"name":"Derik Evangelista","title":"Engineer @ Syntasso","url":"https://github.com/kirederik","imageURL":"https://2.gravatar.com/avatar/7ac63fbda18c97f6a7fab8af157021367793187f4c5830eb722ff565c5a767e9?size=256","key":"derik","page":null}],"frontMatter":{"slug":"compound-promises","title":"How to write Compound Promises","description":"Everything you need to know about building Compound Promises effectively","authors":["derik"],"tags":["kratix","promises","compound promise"]},"unlisted":false,"prevItem":{"title":"Debugging in Kratix","permalink":"/blog/debugging-promise-updates"},"nextItem":{"title":"How your Resources get from Promise to Destination","permalink":"/blog/from-platform-to-destination"}},"content":"So you read the [guide](/main/guides/compound-promises) on Compound Promises and tried out the Workshop, and decided that a compound promise is the right abstraction to expose in your platform. You are about to start writing it, but you are still wondering how you would _really_ go about writing one.\\n\\nWe hear you.\\n\\nIn this blog post, we will build a Compound Promise from scratch. Consider this the ultimate guide on how to build compound promises effectively.\\n\\nYou can follow this guide and build the Promise along with us, or you can use it as a reference when building your own Compound Promises. The Promise we will build is available [here](https://github.com/syntasso/kratix-docs/tree/main/assets/rubyapp-promise).\\n\\nAfter reading this post you will:\\n\\n* Learn about some basic Kratix concepts\\n* Learn how to write a Compound Promise\\n  * By transforming an user\'s request into a series of sub-requests\\n  * By sending those sub-requests to the Platform cluster (and why you need it)\\n  * By defining the sub-Promises that the parent Promise depends on\\n\\nClick on \\"read more\\" to continue!\\n\\n{/* truncate */}\\n\\n---\\n\\nBefore we start, let\'s clarify a few words we will use in abundance throughout this article:\\n\\n* **Promise**: The basic building block in Kratix. A Promise defines something-as-a-service. If you\'re unfamiliar with Promises, we recommend reading our documentation, such as [Installing and using a Promise](/main/guides/installing-a-promise).\\n* **Workflow**: defined within a Promise, it transforms the user\'s request into reality.\\n* **Compound Promise**: a Promise that orchestrates one or more Promises. We may refer to it as \\"the parent promise\\" or \\"the super promise\\" in this article.\\n  * The Promises that a Compound Promise is orchestrating may be referred to as \\"child promises\\" or \\"sub-promises\\".\\n\\nNow that we understand each other, let\'s dive into, well, the reason you are here: building a compound promise. I\'m going to split this article into the following sections:\\n\\n* Defining the user experience.\\n* Building the Compound Promise.\\n  * Implementing the API.\\n  * Implementing the workflows.\\n  * Scheduling to the Platform Cluster.\\n  * Defining the Promise requirements.\\n* Installing the Promise\\n* Testing it all together\\n\\nLet\'s jump right in!\\n\\n## Defining the user experience\\n\\nAs previously mentioned, a compound promise orchestrates one or more promises to provide a higher-level developer experience. Imagine a platform like Heroku or Fly.io: they make it very easy for developers to get off the ground and quickly get their applications up and running in the cloud. On the other hand, they still give users the necessary hooks to tweak configurations so they can get exactly what they need.\\n\\nFor example, to deploy a Rails application to [fly.io](https://fly.io/docs/rails/getting-started/existing/), all the user needs to do is execute a command using the `flyctl` CLI. That command Will interactively ask the user for inputs, like if they need a PostgreSQL database and a Redis cache to be deployed with the application, and, depending on their answers, it will create and deploy the necessary resources. It will also make sure that the running application is wired correctly to the services. Once everything is created, the user will get back an URL with the address of their running application (and services).\\n\\nKratix makes building a similar experience in your internal platform as easy. While it is possible to build all of that in a single Promise, a better approach is to build smaller, single-responsibility Promises\u2013like a dedicated PostgreSQL Promise\u2013and then create a Promise at a higher-level of abstraction that orchestrates requesting the lower-level resources. That higher-level Promise is what we are going to build.\\n\\nAnd what will this Promise do? Well, provide the exact same experience to deploy Rails applications as fly.io:\\n\\n* Given an container image of an application, the Promise should deploy it.\\n* If required by the user, they can also deploy:\\n  * A PostgreSQL database.\\n  * A Redis cache.\\n* The Promise should wire the application with the connection details of the optional services.\\n\\nLuckily, all the sub-promises you need to build this Compound Promise are available in the [Kratix Marketplace](/marketplace):\\n* The Runtime Promise can deploy applications;\\n* The PostgreSQL Promise can deploy PostgreSQL databases;\\n* The Redis Promise can be our cache provider;\\n\\nAll the Compound Promise\u2014let\'s call it the RubyApp Promise\u2014needs to do is orchestrate requests to those other promises.\\n\\n\\n```mdx-code-block\\nimport Figure00 from \\"./figure00.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure00} alt=\\"The RubyApp Promise interacting with sub-promises\\" />\\n\\n  <figcaption>RubyApp Promise orchestrating the sub-promises</figcaption>\\n</figure>\\n\\nIn order to create this experience, we must start by considering what\'s going to be the RubyApp Promise API: What can the users configure? What\'s the right level of abstraction? Do we give more configuration options and risk it being too complex, or keep it high-level and risk it being too limiting?\\n\\nAs with many good questions in life, the answer is: _it depends_. Circling back to the developer experience we want to provide, there are benefits in keeping things as simple as possible. The beauty of Compound Promises is that users can still directly use lower-level Promises. This characteristic allows Platform engineers to offer multiple ways of consuming services. The 80/20 rule is a good principle to keep in mind:\\n\\n* What would be an API that would satisfy 80% of the use-cases of the RubyApp Promise?\\n* The remaining 20% could consume the lower-level Promises directly.\\n\\nAlright, with that in mind, what should we include in the API?\\n\\nAt the most basic level, we need the application to run. Since we will run it in Kubernetes, this could be provided as a container image. We also need a way for users to specify whether they require a database or a cache (or both).\\n\\nSo our API is starting to form. Something like this may be enough to get us started:\\n\\n```yaml\\nimage: container/myapp:v1.0.0\\ndatabase: true\\ncache: false\\n```\\n\\nHowever, thinking a bit ahead, we can imagine a scenario where users would want a different type of database (like MySQL instead of PostgreSQL), or provide some extra configuration for the it. A better API would leave the options opened, so in the future we could expand on the options. Something like this would be better:\\n\\n```yaml\\nimage: myorg/ruby-app:v1.0.0\\ndatabase:\\n  driver: postgresql\\ncache:\\n  driver: redis\\n```\\n\\nIf we translate this to a resource request of the RubyApp Promise, it may look like this:\\n\\n```yaml\\napiVersion: internal.platform.io/v1\\nkind: RubyApp\\nmetadata:\\n  name: my-app\\n  namespace: default\\nspec:\\n  image: myorg/ruby-app:v1.0.0\\n  database:\\n    driver: postgresql\\n  cache:\\n    driver: redis\\n```\\n\\nWith the experience defined, we can now start building the Promise.\\n\\n## Building the Compound Promise\\n\\n:::tip[Skip the build!]\\n\\nYou can follow this guide and build the Promise along with us, or you can use it as a reference when building your own Compound Promises.\\n\\nThe Promise mentioned in this post is available [here](https://github.com/syntasso/kratix-docs/tree/main/assets/rubyapp-promise).\\n\\nYou can skip straight to [installing the Promise](#install) if you want to see it in action.\\n\\n:::\\n\\n### Implementing the API\\n\\nWith the API and experience defined, let\'s create the RubyApp Promise. We can use the [Kratix CLI](/main/kratix-cli/intro) to speed up development. Create a new directory in your system and initialise a new Promise:\\n\\n```bash\\nmkdir rubyapp-promise && cd rubyapp-promise\\n\\nkratix init promise rubyapp \\\\\\n  --group internal.platform.io \\\\\\n  --kind RubyApp \\\\\\n  --version v1\\n```\\n\\nThe command above should produce a `promise.yaml` in the `rubyapp-promise` directory. We can now add the API properties we defined above:\\n\\n```bash\\nkratix update api \\\\\\n  --property image:string \\\\\\n  --property database.driver:string \\\\\\n  --property cache.driver:string\\n```\\n\\nThe next step is to implement the workflow that will transform the user\'s request into the sub-promises request.\\n\\n### Implementing the Workflow\\n\\nTo quick-start the workflow, run the `kratix add container` command:\\n\\n```bash\\nkratix add container resource/configure/instance \\\\\\n  --image ghcr.io/syntasso/kratix-docs/rubyapp-promise:v1.0.0 \\\\\\n  --name deploy-resources\\n```\\n\\nAt this point, your local `rubyapp-promise` directory should look like this:\\n\\n```\\n.\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 example-resource.yaml\\n\u251c\u2500\u2500 promise.yaml\\n\u2514\u2500\u2500 workflows\\n    \u2514\u2500\u2500 resource\\n        \u2514\u2500\u2500 configure\\n            \u2514\u2500\u2500 instance\\n                \u2514\u2500\u2500 deploy-resources\\n                    \u251c\u2500\u2500 Dockerfile\\n                    \u251c\u2500\u2500 resources\\n                    \u2514\u2500\u2500 scripts\\n                        \u2514\u2500\u2500 pipeline.sh\\n\\n8 directories, 5 files\\n```\\n\\nA quick recap of what the pipeline script should do:\\n\\n1. Deploy the application specified with the specified image, via the Runtime Promise\\n1. Create a PostgreSQL instance if `database.driver` is set to `postgresql`, via the PostgreSQL Promise\\n1. Create a Redis instance if `cache.driver` is set to `redis`, via the Redis Promise.\\n1. Update the application environment variables with the credentials for the optional services.\\n\\n#### Adding the Runtime Request\\n\\nLet\'s start from the start and add the first requirement. To deploy the application via the Runtime Promise, the pipeline must output a Resource Request for that Promise. The [API for the Runtime Promise](https://github.com/syntasso/kratix-marketplace/blob/main/runtime/resource-request.yaml) looks like this:\\n\\n```yaml\\napiVersion: marketplace.kratix.io/v1alpha1\\nkind: Runtime\\nmetadata:\\n  name: example-runtime\\n  namespace: default\\nspec:\\n  lifecycle: dev\\n  image: syntasso/website\\n  servicePort: 80\\n  replicas: 1\\n  applicationEnv:\\n  - name: hello\\n    value: from-env\\n```\\n\\nAs you can see, the only configuration option we are currently providing in the API of our RubyApp promise is the image. All the other fields should either be populated by the RubyApp workflow, or left empty. We must also ensure the `metadata.name` we generate for this request is unique, otherwise it may clash with other resources already deployed.\\n\\nSince our pipeline script will be a bit complex, let\'s implement it using Ruby. For that, open the `Dockerfile` in `workflows/resource/configure/instance/deploy-resources/` and add `ruby` to your container. You should change the extension of the `pipeline.sh` to `pipeline.rb` as well.\\n\\nThe resulting Dockerfile will look like this:\\n\\n```dockerfile\\nFROM \\"alpine\\"\\n\\nRUN apk update && apk add --no-cache yq ruby\\n\\nADD scripts/pipeline.rb /usr/bin/pipeline.rb\\nADD resources resources\\n\\nRUN chmod +x /usr/bin/pipeline.rb\\n\\nCMD [ \\"sh\\", \\"-c\\", \\"pipeline.rb\\" ]\\nENTRYPOINT []\\n```\\n\\nUpdate the extension of the pipeline script in your filesystem:\\n\\n```bash\\nmv workflows/resource/configure/instance/deploy-resources/scripts/pipeline.{sh,rb}\\n```\\n\\nNow open the `pipeline.rb` script in `workflows/resource/configure/instance/deploy-resources/scripts` and update it to:\\n\\n```ruby\\n#!/usr/bin/env ruby\\n\\nrequire \'yaml\'\\n\\n# Read the input YAML file\\ninput_yaml = YAML.load_file(\'/kratix/input/object.yaml\')\\n\\n# Extract values from input\\napp_name = input_yaml[\'metadata\'][\'name\']\\nnamespace = input_yaml[\'metadata\'][\'namespace\']\\napp_image = input_yaml[\'spec\'][\'image\']\\n\\n# Create the Runtime request\\nruntime_request = {\\n  \'apiVersion\' => \'marketplace.kratix.io/v1alpha1\',\\n  \'kind\' => \'Runtime\',\\n  \'metadata\' => {\\n    \'name\' => app_name,\\n    \'namespace\' => namespace\\n  },\\n  \'spec\' => {\\n    \'image\' => app_image,\\n    \'replicas\' => 1,\\n    \'servicePort\' => 80,\\n    \'applicationEnv\' => [\\n      { \'name\' => \'PORT\', \'value\' => \'80\' }\\n    ]\\n  }\\n}\\n\\n# Write to Runtime request to the output file\\nFile.write(\'/kratix/output/runtime-request.yaml\', runtime_request.to_yaml)\\n```\\n\\nAs you can see, we have hidden away from the RubyApp user a few options the Runtime Promise provides, like replicas and service port. In your own organisation, those options may need to be exposed at the higher-level Promise.\\n\\n#### Adding the PostgreSQL Request\\n\\nNext step is to optionally include a request to the PostgreSQL Promise if the user requested a database. The API for the PostgreSQL Promise looks like this:\\n\\n```yaml\\napiVersion: marketplace.kratix.io/v1alpha1\\nkind: postgresql\\nmetadata:\\n  name: example\\n  namespace: default\\nspec:\\n  env: dev\\n  teamId: acid\\n  dbName: bestdb\\n```\\n\\nBack in the pipeline script, let\'s update it to include this request when needed. We should also make sure to include the connection details as environment variables to our Runtime request, so the application can connect. Add the following code snippet just after the `runtime_request` assignment:\\n\\n\\n```ruby\\n# ...\\n\\ndatabase_driver = input_yaml.dig(\'spec\', \'database\', \'driver\')\\n\\nif database_driver == \\"postgresql\\" then\\n  # The PostgreSQL Request\\n  database_request = {\\n    \'apiVersion\' => \'marketplace.kratix.io/v1alpha1\',\\n    \'kind\' => \'postgresql\',\\n    \'metadata\' => {\\n      \'name\' => app_name + \'-db\',\\n      \'namespace\' => namespace\\n    },\\n    \'spec\' => {\\n      \'env\' => \'dev\',\\n      \'teamId\' => app_name,\\n      \'dbName\' => app_name + \'-db\'\\n    }\\n  }\\n\\n  # This is the secret name the PostgreSQL promise will generate\\n  secret_name=\\"#{app_name}.#{app_name}-#{app_name}-db-postgresql.credentials.postgresql.acid.zalan.do\\"\\n\\n  ## Injecting the secrets into the application env\\n  runtime_request[\'spec\'][\'applicationEnv\'].push({\\n    \'name\' => \'PGHOST\',\\n    \'value\' => \'${app_name}-${app_name}-db-postgresql.default.svc.cluster.local\'\\n  }, {\\n    \'name\' => \'DBNAME\',\\n    \'value\' => \'${app_name}-db\'\\n  }, {\\n    \'name\' => \'PGUSER\',\\n    \'valueFrom\' => {\\n      \'secretKeyRef\' => { \'name\' => secret_name, \'key\' => \'username\' }\\n    }\\n  }, {\\n      \'name\' => \'PGPASSWORD\',\\n      \'valueFrom\' => {\\n        \'secretKeyRef\' => { \'name\' => secret_name, \'key\' => \'password\' }\\n      }\\n    }\\n  )\\n\\n  File.write(\'/kratix/output/postgresql-request.yaml\', database_request.to_yaml)\\nend\\n```\\n\\n:::tip[Where is the connection details coming from?]\\n\\nThe PostgreSQL Promise generates a Secret and a Service as part of its resource workflow. The Runtime Promise provides the `spec.applicationEnv` property, which allows us to inject environment variables directly into the application.\\n\\nBy combining these two mechanisms, we can seamlessly construct the connection details for the application.\\n\\n:::\\n\\n#### Adding the Redis Request\\n\\nFinally, we do the same with the Redis Promise. It\'s API looks like this:\\n\\n```yaml\\napiVersion: marketplace.kratix.io/v1alpha1\\nkind: redis\\nmetadata:\\n  name: example\\n  namespace: default\\nspec:\\n  size: small\\n```\\n\\nSimple. Similar to the PostgreSQL request, when the user requests a cache, we should add the Redis request to the output directory and inject the connection details into the Runtime request. Right after the PostgreSQL block you just added, include the following:\\n\\n```ruby\\n\\ncache_driver = input_yaml.dig(\'spec\', \'cache\', \'driver\')\\n\\nif cache_driver == \\"redis\\" then\\n  redis_request = {\\n    \'apiVersion\' => \'marketplace.kratix.io/v1alpha1\',\\n    \'kind\' => \'redis\',\\n    \'metadata\' => {\\n      \'name\' => app_name + \'-cache\',\\n      \'namespace\' => namespace\\n    },\\n    \'spec\' => {\\n      \'size\' => \'small\'\\n    }\\n  }\\n\\n  runtime_request[\'spec\'][\'applicationEnv\'].push({\\n    \'name\' => \'REDIS_URL\',\\n    \'value\' => \\"redis://rfs-#{app_name}-cache:26379/1\\"\\n  }, {\\n    \'name\' => \'REDIS_POOL_SIZE\',\\n    \'value\' => \'5\'\\n  })\\n\\n  File.write(\'/kratix/output/redis-request.yaml\', redis_request.to_yaml)\\nend\\n```\\n\\nAnd that\'s it. Workflow done!\\n\\n<details>\\n<summary>Click here for the complete `pipeline.rb` script</summary>\\n\\n```ruby\\n#!/usr/bin/env ruby\\n\\nrequire \'yaml\'\\n\\n# Read the input YAML file\\ninput_yaml = YAML.load_file(\'/kratix/input/object.yaml\')\\n\\n# Extract values from input\\napp_name = input_yaml[\'metadata\'][\'name\']\\nnamespace = input_yaml[\'metadata\'][\'namespace\']\\napp_image = input_yaml[\'spec\'][\'image\']\\n\\n# Create the Runtime request\\nruntime_request = {\\n  \'apiVersion\' => \'marketplace.kratix.io/v1alpha1\',\\n  \'kind\' => \'Runtime\',\\n  \'metadata\' => {\\n    \'name\' => app_name,\\n    \'namespace\' => namespace\\n  },\\n  \'spec\' => {\\n    \'image\' => app_image,\\n    \'replicas\' => 1,\\n    \'servicePort\' => 80,\\n    \'applicationEnv\' => [\\n      { \'name\' => \'PORT\', \'value\' => \'80\' }\\n    ]\\n  }\\n}\\n\\ndatabase_driver = input_yaml.dig(\'spec\', \'database\', \'driver\')\\n\\nif database_driver == \\"postgresql\\" then\\n  # The PostgreSQL Request\\n  database_request = {\\n    \'apiVersion\' => \'marketplace.kratix.io/v1alpha1\',\\n    \'kind\' => \'postgresql\',\\n    \'metadata\' => {\\n      \'name\' => app_name + \'-db\',\\n      \'namespace\' => namespace\\n    },\\n    \'spec\' => {\\n      \'env\' => \'dev\',\\n      \'teamId\' => app_name,\\n      \'dbName\' => app_name + \'-db\'\\n    }\\n  }\\n\\n  # This is the secret name the PostgreSQL promise will generate\\n  secret_name=\\"#{app_name}.#{app_name}-#{app_name}-db-postgresql.credentials.postgresql.acid.zalan.do\\"\\n\\n  ## Injecting the secrets into the application env\\n  runtime_request[\'spec\'][\'applicationEnv\'].push({\\n    \'name\' => \'PGHOST\',\\n    \'value\' => \'${app_name}-${app_name}-db-postgresql.default.svc.cluster.local\'\\n  }, {\\n    \'name\' => \'DBNAME\',\\n    \'value\' => \'${app_name}-db\'\\n  }, {\\n    \'name\' => \'PGUSER\',\\n    \'valueFrom\' => {\\n      \'secretKeyRef\' => { \'name\' => secret_name, \'key\' => \'username\' }\\n    }\\n  }, {\\n      \'name\' => \'PGPASSWORD\',\\n      \'valueFrom\' => {\\n        \'secretKeyRef\' => { \'name\' => secret_name, \'key\' => \'password\' }\\n      }\\n    }\\n  )\\n\\n  File.write(\'/kratix/output/postgresql-request.yaml\', database_request.to_yaml)\\nend\\n\\ncache_driver = input_yaml.dig(\'spec\', \'cache\', \'driver\')\\n\\nif cache_driver == \\"redis\\" then\\n  redis_request = {\\n    \'apiVersion\' => \'marketplace.kratix.io/v1alpha1\',\\n    \'kind\' => \'redis\',\\n    \'metadata\' => {\\n      \'name\' => app_name + \'-cache\',\\n      \'namespace\' => namespace\\n    },\\n    \'spec\' => {\\n      \'size\' => \'small\'\\n    }\\n  }\\n\\n  runtime_request[\'spec\'][\'applicationEnv\'].push({\\n    \'name\' => \'REDIS_URL\',\\n    \'value\' => \\"redis://rfs-#{app_name}-cache:26379/1\\"\\n  }, {\\n    \'name\' => \'REDIS_POOL_SIZE\',\\n    \'value\' => \'5\'\\n  })\\n\\n  File.write(\'/kratix/output/redis-request.yaml\', redis_request.to_yaml)\\nend\\n\\n# Write to Runtime request to the output file\\nFile.write(\'/kratix/output/runtime-request.yaml\', runtime_request.to_yaml)\\n```\\n\\n</details>\\n\\nThere are only two things left to do in our Compound Promise:\\n\\n* Ensure the outputs of the pipeline are scheduled to the Platform cluster\\n* Set the sub-Promises as requirements for the Compound Promise\\n\\nThe next sections will explore how to do this.\\n\\n### Scheduling to the Platform cluster\\n\\nLet take a moment to revisit the behaviour of installing a normal Promise. When a Promise is applied on the Platform cluster, Kratix ensures the API defined within the Promise becomes available in the Platform as a CRD, which enable users to make request to those Promises. The Promise dependencies, on the other hand, are installed on any Destination that could run the workloads.\\n\\nThe Runtime Promise we will use, for example, has a dependency on the Nginx Controller. When that promise is applied, that dependency is installed into any Destination that can receive Runtime instances. When a user requests a new instance, they use the Runtime Promise API to trigger the workflows, that will in turn generate the documents that will be scheduled to the Destination.\\n\\n```mdx-code-block\\nimport Figure02 from \\"./figure02.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure02} alt=\\"The Runtime Promise\\" />\\n\\n  <figcaption>The Runtime Promise and it\'s point of interaction</figcaption>\\n</figure>\\n\\nCompound Promises behave the exact same way: in response of a user\'s request, a workflow is executed and a set of documents are generated. Those documents are stored in the State Store to be picked up by a GitOps agent. The difference here is that those documents are themselves requests for other Promises. That means that the Cluster reconciling on the State Store must be able to understand the CRD of the sub-Promises. In most cases, that means scheduling the documents to the Platform cluster itself.\\n\\n\\n```mdx-code-block\\nimport Figure03 from \\"./figure03.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure03} alt=\\"The Platform cluster reconciling on the State store\\" />\\n\\n  <figcaption>Scheduling documents to the Platform cluster</figcaption>\\n</figure>\\n\\nTo ensure that the documents generated by the RubyApp Promise are scheduled to the Platform cluster, we need to:\\n\\n1. Create a Destination representing the platform with some specific labels, like `environment=platform`\\n2. Configure the GitOps agent in the Platform cluster\\n3. Add Destination Selectors in the Compound Promise.\\n\\nWe won\'t go into detail about (1) and (2) in this blog post. You can find more information about how to [Registering the Platform as a Destination](/workshop/multiple-promises#register-the-platform-as-a-destination) in the Kratix workshop.\\n\\n:::tip\\n\\nTo quickly get an environment compatible with the promises in this blog post, clone Kratix and run:\\n\\n```bash\\nmake quick-start\\nmake prepare-platform-as-destination\\n```\\n\\n:::\\n\\nAt this stage, you should see the following when listing the Destinations in your Platform cluster:\\n\\n```shell-session\\n$ kubectl --context $PLATFORM get destinations --show-labels\\nNAME               READY   LABELS\\nplatform-cluster   True    environment=platform\\nworker-1           True    environment=dev\\n```\\n\\nFor (3), open your Promise file and, under `spec`, add the following:\\n\\n```yaml\\napiVersion: platform.kratix.io/v1alpha1\\nkind: Promise\\nmetadata:\\n  creationTimestamp: null\\n  name: rubyapp\\nspec:\\n  #highlight-start\\n  destinationSelectors:\\n  - matchLabels:\\n      environment: platform\\n  #highlight-end\\n  api: #...\\n```\\n\\n:::tip\\n\\nIt is possible to dynamically generate the destination selectors by creating a `destination-selectors.yaml` file in the `/kratix/metadata/` directory in the Workflow. You can read more about it in [Managing Multiple Destinations](/main/reference/destinations/multidestination-management).\\n\\n:::\\n\\nThe above declaration tells Kratix to schedule the outputs of this Promise to a Destination with the label `environment=platform`. Since we configured the Platform destination with this label, we already have everything in place for the RubyApp Promise to work.\\n\\n\\n### Defining the Promise Requirements\\n\\nThe final piece missing in our Compound Promise is the declaration of the sub-Promises it depends on. For that, you set the `spec.requiredPromises` field in the Compound Promise document with a list of sub-Promise names and versions.\\n\\nLet\'s update our RubyApp Promise to include the required sub-Promises:\\n\\n\\n```yaml\\napiVersion: platform.kratix.io/v1alpha1\\nkind: Promise\\nmetadata:\\n  creationTimestamp: null\\n  name: rubyapp\\nspec:\\n  #highlight-start\\n  requiredPromises:\\n  - name: postgresql\\n    version: v1.0.0-beta.2\\n  - name: redis\\n    version: v0.1.0\\n  - name: runtime\\n    version: v1.0.0\\n  #highlight-end\\n  destinationSelectors: #..\\n  api: #...\\n```\\n\\nWe are now ready to install it!\\n\\n## Installing the Promise {#install}\\n\\nIf we try to install the Compound Promise now, you should get a warning:\\n\\n```shell-session\\n$ kubectl --context $PLATFORM apply --filename promise.yaml\\nWarning: Required Promise \\"postgresql\\" at version \\"v1.0.0-beta.2\\" not installed\\nWarning: Required Promise \\"redis\\" at version \\"v1.0.0-beta.1\\" not installed\\nWarning: Required Promise \\"runtime\\" at version \\"v0.1.0\\" not installed\\nWarning: Promise will not be available until the above issue(s) is resolved\\npromise.platform.kratix.io/rubyapp configured\\n```\\n\\nThe Compound Promise itself will remain unavailable until the requirements are satisfied.\\n\\n```shell-session\\n$ kubectl --context $PLATFORM get promises\\nNAME      STATUS        KIND      API VERSION               VERSION\\nrubyapp   Unavailable   RubyApp   internal.platform.io/v1\\n```\\n\\nTo satisfy the `requiredPromises` declaration, you will to, well, install the required promises in your Platform cluster:\\n\\n```bash\\nkubectl --context $PLATFORM apply --filename https://raw.githubusercontent.com/syntasso/promise-postgresql/main/promise-release.yaml\\nkubectl --context $PLATFORM apply --filename https://raw.githubusercontent.com/syntasso/kratix-marketplace/main/redis/promise-release.yaml\\nkubectl --context $PLATFORM apply --filename https://raw.githubusercontent.com/syntasso/kratix-marketplace/main/runtime/promise-release.yaml\\n```\\n\\nAfter a few seconds, you should see all the Promises available in your Platform:\\n\\n```shell-session\\n$ kubectl --context $PLATFORM get promises\\nNAME         STATUS      KIND         API VERSION                      VERSION\\npostgresql   Available   postgresql   marketplace.kratix.io/v1alpha1   v1.0.0-beta.2\\nredis        Available   redis        marketplace.kratix.io/v1alpha1   v0.1.0\\nrubyapp      Available   RubyApp      internal.platform.io/v1\\nruntime      Available   Runtime      marketplace.kratix.io/v1alpha1   v1.0.0\\n```\\n\\n:::tip\\n\\nYou may have noticed that we are applying a different type of resource: a Promise Release. This blog post will not go into detail on the Promise Releases, but you can find more information on them [here](/main/reference/promises/releases).\\n\\n:::\\n\\nYou are now ready to send requests to your Compound Promise!\\n\\n## Testing it all together\\n\\nNow that you have everything set in the Platform, you can go ahead and deploy the your applications!\\n\\nFor that, create a request for your RubyApp Promise:\\n\\n```yaml\\ncat <<EOF | kubectl --context $PLATFORM apply -f -\\napiVersion: internal.platform.io/v1\\nkind: RubyApp\\nmetadata:\\n  name: myapp\\nspec:\\n  image: syntasso/example-rails-app:v1.0.0 # you can use this one, or build your own\\n  database:\\n    driver: postgresql\\n  cache:\\n    driver: redis\\nEOF\\n```\\n\\nSending this request will immediately trigger the RubyApp Promise Resource workflow. That, in turn, should trigger the sub-promises workflows:\\n\\n```shell-session\\n$ kubectl --context $PLATFORM get pods\\nNAME                                                        READY   STATUS      RESTARTS   AGE\\nkratix-postgresql-myapp-db-instance-configure-abcc3-brgbh   0/1     Completed   0          46s\\nkratix-redis-myapp-redis-instance-configure-d2c53-rqf8s     0/1     Completed   0          46s\\nkratix-rubyapp-myapp-instance-c87d1-k892w                   0/1     Completed   0          53s\\nkratix-runtime-myapp-instance-2ecbc-2lstz                   0/1     Completed   0          45s\\n```\\n\\nIn a couple of minutes, in your Worker cluster, you should see the application pod running, alongside the Redis and PostgreSQL databases:\\n\\n```shell-session\\nNAME                                              READY   STATUS        RESTARTS   AGE\\nmyapp-7c7cffcc5f-7wrdd                            1/1     Running       0          33s\\nmyapp-myapp-db-postgresql-0                       1/1     Running       0          32s\\nrfr-myapp-redis-0                                 1/1     Running       0          33s\\nrfs-myapp-redis-5cb45649b4-mx5wq                  1/1     Running       0          33s\\n# other pods\\n```\\n\\nAnd you can now access your application:\\n\\n\\n:::tip\\n\\nIf you used the quick-start command to set up your environment, you can access the application at http://myapp.default.local.gd:31338/. Otherwise, you may need to port-forward to the application pod.\\n\\nYou also may need a couple of refreshes to get the green checks, as the database and cache may take a few seconds to be ready.\\n\\n:::\\n\\n```mdx-code-block\\nimport RunningApp from \\"./running-app.png\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={RunningApp} alt=\\"Rails application running with PostgreSQL and Redis \\" />\\n\\n  <figcaption>The running Rails App</figcaption>\\n</figure>\\n\\n\ud83c\udf89 The App is up-and-running! The RubyApp Promise has successfully orchestrated the provisioning of the PostgreSQL and Redis databases, and the deployment of the application. It then wired the application to the databases by injecting the connection details into the environment variables.\\n\\n## Conclusion\\n\\nWe\'ve just taken a deep dive into building a Compound Promise from the ground up. From defining a user-centric experience to orchestrating sub-promises, we\'ve walked through each critical step of creating flexible platform abstractions.\\n\\nThe magic of Compound Promises lies not just in their technical implementation, but in their ability to abstract away complexity while keeping extensibility at the forefront. The RubyApp Promise we built today is just the beginning\u2014imagine the platforms you could create!\\n\\nI hope this post gives you a good base to build your own developer experiences with Compound Promises. As always, feel free to drop by our [Slack](https://kratix.slack.com) or [GitHub](https://github.com/syntasso/kratix) to continue the conversation."},{"id":"from-platform-to-destination","metadata":{"permalink":"/blog/from-platform-to-destination","source":"@site/blog/2024-11-20-decompressing-work/index.mdx","title":"How your Resources get from Promise to Destination","description":"The journey of a document from the Platform Cluster to a Destination","date":"2024-11-20T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"kratix internals","permalink":"/blog/tags/kratix-internals"},{"inline":true,"label":"debugging","permalink":"/blog/tags/debugging"}],"readingTime":6.47,"hasTruncateMarker":true,"authors":[{"name":"Derik Evangelista","title":"Engineer @ Syntasso","url":"https://github.com/kirederik","imageURL":"https://2.gravatar.com/avatar/7ac63fbda18c97f6a7fab8af157021367793187f4c5830eb722ff565c5a767e9?size=256","key":"derik","page":null}],"frontMatter":{"slug":"from-platform-to-destination","title":"How your Resources get from Promise to Destination","description":"The journey of a document from the Platform Cluster to a Destination","authors":["derik"],"tags":["kratix","kratix internals","debugging"]},"unlisted":false,"prevItem":{"title":"How to write Compound Promises","permalink":"/blog/compound-promises"},"nextItem":{"title":"September Product Update","permalink":"/blog/sept-2024-product-update"}},"content":"```mdx-code-block\\nimport Figure01 from \\"./figure01.png\\"\\nimport Figure02 from \\"./figure02.png\\"\\nimport Figure03 from \\"./figure03.png\\"\\nimport Figure04 from \\"./figure04.png\\"\\n```\\n\\nEver wondered how Kratix actually gets your documents from the Platform Cluster\\nto the correct Destination?\\n\\nThe Syntasso Team has recently introduced a\\n[change](https://github.com/syntasso/kratix/pull/243) to\\nKratix to reduce the size of the Work object. While this change is mostly\\ninternal, we wanted to share how the innards of Kratix work.\\n\\nSo brace yourself to learn:\\n- how Kratix moves documents from Platform to Destinations\\n- what works and workplacements are\\n- how to inspect works to debug your Promises\\n\\nYou are probably already familiar with how Kratix works at a high level and with\\nthe diagram below:\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure01} alt=\\"High level diagram explaining how\\nKratix processes requests\\" />\\n\\n  <figcaption>How Kratix processes a request to a Kubernetes Destination</figcaption>\\n</figure>\\n\\nAs illustrated above:\\n\\n1. The user sends a new **App Request** to the Platform Cluster.\\n2. The **Promise** reacts to that request and triggers the **Resource Configure\\n   Workflows**.\\n3. The Workflow completes and outputs a **series of documents** to be scheduled\\n   to a **Destination**.\\n4. These documents are written to a specific directory in the **State Store**.\\n5. In the diagram, the documents are scheduled to a Kubernetes **Destination**.\\n   These type of Destination usually have Flux (or ArgoCD, or another GitOps\\n   tool) watching the State Store. The tool picks up the new documents.\\n6. The documents are then processed and applied to the Destination.\\n7. The **App** becomes operational on the Destination.\\n\\nIn this post, I\'m going to expand on points (3) and (4): what happens at the end\\nof the Workflow? How is the document written to the State Store? And how does\\nthe change linked above affect this process?\\n\\n---\\n\\n{/* truncate */}\\n\\n:::tip\\n\\nIf the diagram is new to you, I recommend checking out the [Part I of the\\nKratix Workshop](http://localhost:3000/workshop/part-i/intro) for an overview of\\nKratix.\\n\\n:::\\n\\n## A Dive into Kratix Internals\\n\\nThe casual observers among you may have noticed that, when installing Kratix, a\\ncouple of CRDs are also created but not prominently mentioned in the guides\\nor workshops: the **Work** and the **WorkPlacement**.\\n\\nThe **Work** CRD contains the definition of, well, a Work. All the documents\\noutput by a workflow are captured in the Work Object as **workloads**. Each\\ndocument corresponds to a workload entry in the Work object. These workloads are\\ngrouped by the **destination selectors** specified by both the Workflow and the\\nPromise.\\n\\nIn other words, the Work object encapsulates everything needed to schedule the\\nworkloads to the Destinations.\\n\\n:::info How does the Work gets created?\\n\\nKeen observers may have notice the few extra containers that are included in the\\nWorkflow Job. One of these containers is called `work-writer`. As the name\\nsuggests, it handles creating the Work object at the end of the Workflow. \ud83d\ude09\\n\\n:::\\n\\nOne of the controllers bundled with Kratix is the **Work Controller**. This\\ncontroller is responsible for finding out all the available Destinations and\\nselecting the right one for each workload in a Work. It achieves this by\\nmonitoring Work objects and creating a **WorkPlacement** object for each\\nworkload.\\n\\n:::tip What if there\'s no Destination to schedule a workload?\\n\\nThe Work Controller marks the Work as **Unscheduled**. You can verify this by\\nchecking the `Scheduled` condition in the `status` field of the Work Object.\\n\\nOnce a Destination becomes available, the system will automatically try to\\nschedule any unscheduled Work.\\n\\n:::\\n\\nThe **WorkPlacement** object serves as a link between a Work (or specifically, a\\nworkload group within the Work) and a Destination. It contains a copy of the\\nWorkloads and information about the Destination it is scheduled to.\\n\\nThe WorkPlacement controller reacts to WorkPlacements and ensures the workloads\\nare written to the State Store associated with the Destination.\\n\\nThe diagram below illustrates the Work and WorkPlacement objects in details:\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure02} alt=\\"A Work with two workloads, from\\n    which two WorkPlacements are generated\\" />\\n\\n  <figcaption>A Work generating two WorkPlacements</figcaption>\\n</figure>\\n\\nIn summary:\\n\\n* The contents of a Workflow output are combined into a single Work object. Each\\n  document has an associated `workload` entry in the Work.\\n* Workloads are grouped by the destination selectors specified by the Workflow\\n  and the Promise.\\n* From the Work object, a WorkPlacement is created for each Workload group.\\n* The WorkPlacement controller writes the Workloads to the State Store\\n  associated with the Destination.\\n* \ud83c\udf89\\n\\nThat means the Work object can get quite large, since it\'s combining all the\\ndocuments into a single object. But how large is too large?\\n\\n## Reaching etcd limits\\n\\nThe answer is about 1.5MB. While the Kubernetes API accepts up to 3MB of data in\\na single request, etcd only persist keys up to 1.5MB (by default). Although this\\nis configurable, it\'s fair to assume that most clusters where Kratix is deployed\\nwill use the default settings.\\n\\nSo what happens if a Work object exceeds 1.5MB? The Configure Workflow fails at\\nthe `work-writer` container, and the error message isn\'t particularly helpful:\\n\\n```shell-session\\netcdserver: request is too large\\n```\\n\\n:::tip\\n\\nYou may see an error message like `Request entity too large: limit is\\n3145728`; that means you are hitting the Kubernetes API limit, not the etcd\\none.\\n\\n:::\\n\\nWhile it takes a lot of YAML to be over 1.5MB, you can easily reach such a limit\\nin your Promise. The [Prometheus\\nOperator](https://github.com/prometheus-operator/prometheus-operator), for\\nexample, includes 3.7MB of YAML!\\n\\nThis brings us back to the [change introduced by\\n243](https://github.com/syntasso/kratix/pull/243). In this update, we introduced\\ngzip compression for the Workload contents before persisting the Work into etcd.\\nThis significantly reduces the size of the Work object (gzip documentation\\nmentions an average of [70% reduction in\\nsize](https://www.gnu.org/software/gzip/manual/gzip.html)). For the Prometheus\\nOperator, for example, the size of the Work object went from 3.7MB to about\\n490KB\u2014an 87% reduction \ud83c\udf89!\\n\\nThe downside? If you inspect the Work object, you\u2019ll see base64-encoded binary\\ndata instead of some nice to read YAML.\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure03} alt=\\"Screenshot of a terminal showing\\n  the Work object with binary data in the contents of a Workload\\" />\\n\\n  <figcaption>A compressed Work Object</figcaption>\\n</figure>\\n\\nYou can still read it though. To inspect a workload\u2019s contents, decode the\\nbase64 data, then unzip it using this command:\\n\\n```shell-session\\nkubectl get work <work-name> \\\\\\n  -o jsonpath=\'{.spec.workloadGroups[0].workloads[0].content}\' \\\\\\n  | base64 -d \\\\\\n  | gzip -d\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Figure04} alt=\\"Screenshot of a terminal showing\\n  the decompressed contents of a workload from a Work object\\" />\\n\\n  <figcaption>A decompressed workload</figcaption>\\n</figure>\\n\\n:::tip\\n\\nCheck our [Troubleshooting guide](/main/troubleshooting) for more information on\\nhow to debug Kratix, including inspecting Works and WorkPlacements.\\n\\n:::\\n\\nDespite compression, large Work objects may still pose challenges. While this\\nupdate provides temporary relief, we\u2019ll need to revisit the structure to allow\\nusers to have unbounded fun with their Promises. But that\u2019s a story for another\\nday.\\n\\n## Conclusion\\n\\nIn this post, we dived into the internals of Kratix to understand how a\\ndocument moves from the Platform Cluster to a Destination. We saw how the\\nWork and WorkPlacement objects are used to schedule and write documents to the\\nState Store. We also saw how the recent change to compress the Workload\\ncontents has helped reduce the size of the Work object.\\n\\nWe hope this post has given you a better understanding of how Kratix works under\\nthe hood. If you have any questions or feedback (or want to see more blog posts\\nlike this) please don\'t hesitate to reach out to us on\\n[Slack](https://kratixworkspace.slack.com/) or\\n[GitHub](https://github.com/syntasso/kratix)."},{"id":"sept-2024-product-update","metadata":{"permalink":"/blog/sept-2024-product-update","source":"@site/blog/2024-09-24-product-update/index.mdx","title":"September Product Update","description":"Kratix CLI++, Permissions, and Backstage","date":"2024-09-24T00:00:00.000Z","tags":[{"inline":true,"label":"kratix","permalink":"/blog/tags/kratix"},{"inline":true,"label":"product update","permalink":"/blog/tags/product-update"},{"inline":true,"label":"backstage","permalink":"/blog/tags/backstage"}],"readingTime":4.98,"hasTruncateMarker":true,"authors":[{"name":"Cat Morris","title":"Product Manager @ Syntasso","url":"https://github.com/catmo-syntasso","imageURL":"https://github.com/catmo-syntasso.png","key":"cat","page":null}],"frontMatter":{"slug":"sept-2024-product-update","title":"September Product Update","description":"Kratix CLI++, Permissions, and Backstage","authors":["cat"],"tags":["kratix","product update","backstage"]},"unlisted":false,"prevItem":{"title":"How your Resources get from Promise to Destination","permalink":"/blog/from-platform-to-destination"}},"content":"import ReactPlayer from \'react-player/lazy\';\\n\\nHi friends \ud83d\ude0a I\'m Cat, Product Manager here at Syntasso, and I\'ve been listening\\nto you, our avid followers. \\"Cat, we love Kratix, and we love your team; we want\\nan update!\\" I hear you scream, so here I am, delivering this top-quality\\ncontent. You\'re welcome.\\n\\nGiven that this is our first Kratix Product Update\u2122 (not really \u2122), I wanted to\\nshout about some of the great work the team has done over the last few months,\\nso we\'re going back a bit further than we usually will\u2026 Indulge me; it was my\\nbirthday this month.\\n\\n## TL;DR\\n\\n<ReactPlayer width=\\"100%\\" playsinline wrapper=\\"p\\" controls={false} height={200} playing={true} muted={true} loop={true} url=\\"https://media0.giphy.com/media/Emg9qPKR5hquI/giphy.mp4\\" />\\n\\nIf you only have 10 seconds spare to read this blog, these are the features you\\nneed to know about:\\n\\n* We have two CLIs now! One for building promises and one for installing SKE (Syntasso Kratix Enterprise)\\n* Setting permissions just got a little bit easier\\n* If you like Backstage, you\u2019ll like what we\u2019re up to\\n\\n\x3c!-- truncate --\x3e\\n\\n## Big stuff for the fans of Kratix\\n\\n<ReactPlayer width=\\"100%\\" playsinline height={200} playing={true} muted={true} loop={true} url=\\"https://media0.giphy.com/media/yGEbmgiCJYu3u/giphy.mp4\\" />\\n\\n### Kratix Promise building CLI\\n\\nLike all good tools for developers, we needed a CLI - and one that is more than\\njust a rat nest of bash scripts*. It\'s now even easier to quickly whip together\\na Promise from scratch, or your existing **operators** and **helm charts**.\\n\\n\\nIt\'s unbelievably easy to get started. Head over to our kratix-cli repo,\\ndownload the latest release and go wild. You can read up on the tool in our docs\\nas well!\\n\\nimport CLIDemo from \'./cli-demo.mp4\';\\n\\n<p align=\\"center\\">\\n<ReactPlayer wrapper=\\"span\\" controls url={CLIDemo} />\\n</p>\\n\\n> _That\'s sweet!_\\n>\\n> \u2014 an actual quote from a customer trying out the new CLI.\\n\\n\\n*no offence meant to rats, their nests, or bash scripts that resemble said nests\\n\\n\\n### SKE Operator\\n\\nOur enterprise product, [Syntasso Kratix Enterprise\\n(SKE)](https://www.syntasso.io/pricing), comes with a bunch of cool features\\nthat makes it mega easy to get started, including integrations with Backstage,\\nTerraform Enterprise and now, super speedy installation via our [helm\\nchart](https://docs.kratix.io/ske/kratix/configuring-ske/via-helm) and the\\n[ske-cli](https://docs.kratix.io/ske/kratix/configuring-ske/via-ske-cli).\\n\\nThis will pull in some really cool stuff - it can help manage  upgrading Kratix,\\ncheck if those upgrades worked, and help you with rollbacks.\\n\\nIf you want to try it out, ping us an email at kratix@syntasso.io, and we\u2019ll\\nhook you up \ud83d\ude0e\\n\\nimport SKECli from \'./ske-cli.mp4\';\\n\\n<ReactPlayer wrapper=\\"span\\" muted={true} controls url={SKECli} />\\n\\n## Neat stuff (because security should not be YOLO!)\\n\\n<ReactPlayer width=\\"100%\\" playsinline height={200} playing={true} muted={true} loop={true} url=\\"https://media4.giphy.com/media/HHWdyVKaKJfI8qqJKU/giphy.mp4\\" />\\n\\n### Security uplift\\n\\nSo security is hard. We\u2019ve made this a little bit easier in two ways.\\n\\n#### RBAC permissions for your pipelines\\n\\nYou can set the RBAC permissions you want all your pipelines to have in the promise spec. There are a few ways you can do this depending on your use of service accounts and namespaces, so check out our docs.\\n\\nThis will give you as the promise writer more control of the permissions of your pipelines without having to manually set things up after applying a promise.\\n\\n```yaml\\nplatform: platform.kratix.io/v1alpha1\\nkind: Promise\\nmetadata:\\n  name: env\\nspec:\\n  ...\\n  workflows:\\n    resource:\\n      configure:\\n      - apiVersion: platform.kratix.io/v1alpha1\\n        kind: Pipeline\\n        metadata:\\n          name: slack-notify\\n        spec:\\n          rbac:\\n            permissions:\\n              - apiGroups: [\\"\\"]\\n                verbs: [\\"*\\"]\\n                resources: [\\"secrets\\"]\\n              - apiGroups: [\\"batch\\"]\\n                verbs: [\\"get\\"]\\n                resources: [\\"jobs\\"]\\n                resourceName: [\\"my-job\\"]\\n        ...\\n```\\n\\n#### Security Contexts for Your Pods\\n\\nBy default, Kratix-owned pods have security contexts set with all the privileges they need. Any containers provided by a Promise author got NOTHING. I didn\u2019t say Kratix was a fair project.\\n\\nBut we have just got a little fairer. You can now set security contexts for your own pods by specifying it in the container spec. Perhaps more excitingly, you can now specify a global default security context in the Kratix ConfigMap in the kratix-platform-system. Fire Configure and forget \ud83d\udd25\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: kratix\\n  namespace: kratix-platform-system\\ndata:\\n  config: |\\n    workflows:\\n      defaultContainerSecurityContext:\\n        # Security context fields, e.g.:\\n        runAsNonRoot: false\\n```\\n\\n> _I think it is much better_ \ud83d\udc4d\\n>\\n> \u2014 Another actual customer quote.\\n\\n## More control over Backstage (with Kratix)\\n\\nIf you know anything about Syntasso, it\'s that we love Backstage. We even did a\\nwebinar with one of their product managers, the insightful Seve Kim.\\n\\nWe believe Kratix and Backstage work so beautifully together that our SKE\\noffering comes bundled with plugins that make managing Promises and Resources\\nfrom Backstage a joyful experience. This month, we\'ve given users even more\\ncontrol in two ways.\\n\\nPromise authors can now provide an \\"info\\" field as part of the Promise spec that\\nwill show up on your component page. This field supports markdown and has far\\ntoo much space to let your users know whatever will make their day a little\\neasier.\\n\\nWe have also made the first page of requesting a resource more configurable - we\\nknow not every end user will understand what their \\"namespace\\" should be (even\\nthough it is very important for knowing where we should put that resource!!) so\\nyou can configure that to whatever makes sense for your team. You can even go\\nfull abstraction and get rid of it entirely and pre-populate it with whatever\\nyou want. The sky\'s the limit.\\n\\n```mdx-code-block\\nimport Backstage from \\"./backstage.webp\\"\\n```\\n\\n<figure className=\\"diagram\\">\\n  <img className=\\"large\\" src={Backstage} alt=\\"Screenshot of a Kratix Backstage entity\\" />\\n\\n  <figcaption>Backstage and Kratix: Best of friends</figcaption>\\n</figure>\\n\\n## Useful Kratix resources\\nYou should find the following resources helpful for your Kratix exploration:\\n\\n* [Kratix Github](https://github.com/syntasso/kratix)\\n* [Syntasso Kratix Enterprise homepage](https://www.syntasso.io/)\\n\\nShout out to the team at Port ([getport.io](http://getport.io/)) and Traefik\\n([traefik.io](http://traefik.io/)) for their awesome product update blogs, which\\nserved as inspiration for this post."}]}}')}}]);