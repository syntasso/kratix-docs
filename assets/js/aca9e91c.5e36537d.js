"use strict";(self.webpackChunkkratix_docs=self.webpackChunkkratix_docs||[]).push([[1775],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(96540);const r={},o=i.createContext(r);function s(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(o.Provider,{value:n},e.children)}},45733:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>d,default:()=>m,frontMatter:()=>c,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"ske/guides/promise-from-tf-module","title":"Creating a Promise from a Terraform Module","description":"Step-by-step guide to using the Kratix cli and tfstate-finder to create a Promise from a Terraform Module","source":"@site/docs/ske/05-guides/14-promise-from-tf-module.mdx","sourceDirName":"ske/05-guides","slug":"/ske/guides/promise-from-tf-module","permalink":"/ske/guides/promise-from-tf-module","draft":false,"unlisted":false,"editUrl":"https://github.com/syntasso/kratix-docs/tree/main/docs/ske/05-guides/14-promise-from-tf-module.mdx","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"title":"Creating a Promise from a Terraform Module","description":"Step-by-step guide to using the Kratix cli and tfstate-finder to create a Promise from a Terraform Module","sidebar_label":"Creating a Promise from a Terraform Module","keywords":["terraform","promise"]},"sidebar":"skeSidebar","previous":{"title":"Testing Promises","permalink":"/ske/guides/promise-development"},"next":{"title":"Scheduling to different Terraform Workspaces","permalink":"/ske/guides/scheduling-terraform-workspaces"}}');var r=t(74848),o=t(28453);const s=t.p+"assets/images/terraform-provider-cebf505d60e92062877b9ca11b164a97.png",a=t.p+"assets/images/terraform-plan-6a7e88f8739b5889f1fbcf39e49ad90f.png",l=t.p+"assets/images/terraform-outputs-0c8e0c97f868afaa60e95d43d8c47984.png",c={title:"Creating a Promise from a Terraform Module",description:"Step-by-step guide to using the Kratix cli and tfstate-finder to create a Promise from a Terraform Module",sidebar_label:"Creating a Promise from a Terraform Module",keywords:["terraform","promise"]},d=void 0,u={},h=[{value:"Pre-requisites",id:"pre-requisites",level:2},{value:"Initializing the Promise from the Terraform Module",id:"initializing-the-promise-from-the-terraform-module",level:2},{value:"The API",id:"the-api",level:3},{value:"Destination Selectors",id:"destination-selectors",level:3},{value:"Workflows",id:"workflows",level:3},{value:"Adding the Promise Workflow",id:"adding-the-promise-workflow",level:2},{value:"Validating the generated resources",id:"validating-the-generated-resources",level:2},{value:"Surfacing Terraform outputs",id:"surfacing-terraform-outputs",level:2},{value:"Validating the completed Promise",id:"validating-the-completed-promise",level:3},{value:"\ud83c\udf89 Congratulations",id:"-congratulations",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Terraform Modules are a common starting place for the creation of Promises. We know that a lot of organisations take advantage of Terraform already and the declarative nature of Terraform lends itself well to the definition of declarative Promise workflows. In\nshort: Kratix and Terraform are good friends and our Terraform integrations make then even more compatible."}),"\n",(0,r.jsx)(n.p,{children:"In this guide, we'll be creating a Promise from a Terraform module using both the Kratix CLI and the TF State Finder Pipeline stage. We'll\nbe:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"defining an API that reflects the configurable Terraform values"}),"\n",(0,r.jsx)(n.li,{children:"creating declarative workflows to generate the infrastructure to deploy"}),"\n",(0,r.jsx)(n.li,{children:"surfacing the Terraform outputs in the status of Resource Requests"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"pre-requisites",children:"Pre-requisites"}),"\n",(0,r.jsx)(n.p,{children:"You will need:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["An installation of SKE. Go to ",(0,r.jsx)(n.a,{href:"/ske/installing-ske/intro",children:"Configuring SKE"})," and follow the appropriate guide if you haven't done so\nalready."]}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.a,{href:"/main/kratix-cli/intro",children:"kratix CLI"})]}),"\n",(0,r.jsxs)(n.li,{children:["A Terraform workspace configured to ",(0,r.jsx)(n.a,{href:"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/create",children:"watch a Git\nrepository"}),". As we'll also be creating an AWS resource, this Workspace should have AWS credentials configured."]}),"\n",(0,r.jsxs)(n.li,{children:["A Terraform Destination that is backed by a GitStateStore that references the above repository. You can follow the ",(0,r.jsx)(n.a,{href:"/ske/integrations/tfe#configuring-the-destination",children:"Configuring the Destination"}),"\ndocumentation to set up your Destination."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"initializing-the-promise-from-the-terraform-module",children:"Initializing the Promise from the Terraform Module"}),"\n",(0,r.jsxs)(n.p,{children:["The Terraform Module we'll be using within this workshop is the ",(0,r.jsx)(n.a,{href:"https://github.com/terraform-aws-modules/terraform-aws-s3-bucket",children:"AWS S3 Bucket\nModule"})," which provisions s3 buckets. To get started, let's bootstrap the\nPromise with the ",(0,r.jsx)(n.code,{children:"init tf-module-promise"})," command:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kratix init tf-module-promise s3 \\\n    --module-source "git::https://github.com/terraform-aws-modules/terraform-aws-s3-bucket.git?ref=v5.10.0" \\\n    --group example.syntasso.io \\\n    --kind S3 \\\n    --version v1alpha1 \\\n    --dir s3-promise\n'})}),"\n",(0,r.jsx)(n.h3,{id:"the-api",children:"The API"}),"\n",(0,r.jsxs)(n.p,{children:["Looking closely at the command, the ",(0,r.jsx)(n.code,{children:"init tf-module-promise"})," command generates a Promise from the Terraform module referred to in the ",(0,r.jsx)(n.code,{children:"--module-source"}),".\nIt transforms the variables that can be configured in the module into properties of the Promise API."]}),"\n",(0,r.jsxs)(n.p,{children:["Take a look at the generated ",(0,r.jsx)(n.code,{children:"promise.yaml"})," in the newly created ",(0,r.jsx)(n.code,{children:"s3-promise"})," directory. The ",(0,r.jsx)(n.code,{children:"group"}),", ",(0,r.jsx)(n.code,{children:"version"})," and ",(0,r.jsx)(n.code,{children:"kind"})," mirror those that\nwere specified when initializing the Promise:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: platform.kratix.io/v1alpha1\nkind: Promise\nmetadata:\n  name: s3\n  labels:\n    kratix.io/promise-version: v0.0.1\nspec:\n  api:\n    apiVersion: apiextensions.k8s.io/v1\n    kind: CustomResourceDefinition\n    metadata:\n      name: s3s.example.syntasso.io\n    spec:\n      group: example.syntasso.io\n      names:\n        kind: S3\n        plural: s3s\n        singular: s3\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The S3 Module the Promise was generated from included a ",(0,r.jsx)(n.a,{href:"https://github.com/terraform-aws-modules/terraform-aws-s3-bucket/blob/master/variables.tf",children:"number of\nvariables"}),", which have now been converted to\nproperties in the API. Take the ",(0,r.jsx)(n.code,{children:"create_bucket"})," variable for instance:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'variable "create" {\n  description = "Whether to create an instance"\n  type        = bool\n  default     = true\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["This is now reflected by a property of the same name with same ",(0,r.jsx)(n.code,{children:"type"}),", ",(0,r.jsx)(n.code,{children:"description"})," and ",(0,r.jsx)(n.code,{children:"default"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"  - name: v1alpha1\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            default: {}\n            properties:\n...\n#highlight-start\n              create_bucket:\n                default: true\n                description: Controls if S3 bucket should be created\n                type: boolean\n#highlight-end\n"})}),"\n",(0,r.jsx)(n.h3,{id:"destination-selectors",children:"Destination Selectors"}),"\n",(0,r.jsxs)(n.p,{children:["As the Promise will be generating Terraform configurations, the generated files must be scheduled to a destination that can handle these\nfiles. In this case, the Terraform destination created as part of the ",(0,r.jsx)(n.a,{href:"#pre-requisites",children:"pre-requisites"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["In keeping with this, you'll see that the promise has ",(0,r.jsx)(n.code,{children:"destinationSelectors"})," that ensure workflows are written to a destination with the\nlabel ",(0,r.jsx)(n.code,{children:"environment: terraform"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"  destinationSelectors:\n    - matchLabels:\n        environment: terraform\n"})}),"\n",(0,r.jsx)(n.h3,{id:"workflows",children:"Workflows"}),"\n",(0,r.jsxs)(n.p,{children:["You'll find that a ",(0,r.jsx)(n.code,{children:"resource.configure"})," workflow has also been added to the ",(0,r.jsx)(n.code,{children:"promise.yaml"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"  workflows:\n    resource:\n      configure:\n        - apiVersion: platform.kratix.io/v1alpha1\n          kind: Pipeline\n          metadata:\n            name: instance-configure\n          spec:\n            containers:\n            - env:\n              - name: MODULE_SOURCE\n                value: git::https://github.com/terraform-aws-modules/terraform-aws-s3-bucket.git?ref=v5.10.0\n              image: ghcr.io/syntasso/kratix-cli/terraform-generate:v0.4.0\n              name: terraform-generate\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"ghcr.io/syntasso/kratix-cli/terraform-generate:v0.4.0"})," image takes the inputs provided via the API and uses these to generate a\nTerraform configuration that honours these inputs."]}),"\n",(0,r.jsx)(n.h2,{id:"adding-the-promise-workflow",children:"Adding the Promise Workflow"}),"\n",(0,r.jsx)(n.p,{children:"As the S3 Module has a required Provider, we must ensure that the declaration of the provider is present in the repository that is watched by Terraform. As this only needs to be declared once, we can make it a dependency of Promise, ensuring that the Provider block is present before any files are generated by Resource Requests."}),"\n",(0,r.jsxs)(n.p,{children:["We can add this dependency via the ",(0,r.jsx)(n.code,{children:"update dependencies"})," command."]}),"\n",(0,r.jsxs)(n.p,{children:["Create a ",(0,r.jsx)(n.code,{children:"dependencies"})," directory, and within it, add the following ",(0,r.jsx)(n.code,{children:"provider.tf"})," file:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'terraform {\n  required_providers {\n    aws = {\n      source  = "hashicorp/aws"\n      version = "~> 6.0"\n    }\n  }\n}\n\nprovider "aws" {\n  region = "us-east-1"\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"You can create a Configure Pipeline that takes these dependencies and writes them to your Destination with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kratix update dependencies ./dependencies/ --image kratix-workshop/s3-promise-pipeline:v0.1.0\n"})}),"\n",(0,r.jsx)(n.p,{children:"Your directory structure will now look like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 dependencies\n\u2502\xa0\xa0 \u2514\u2500\u2500 providers.tf\n\u251c\u2500\u2500 example-resource.yaml\n\u251c\u2500\u2500 promise.yaml\n\u2514\u2500\u2500 workflows\n    \u2514\u2500\u2500 promise\n        \u2514\u2500\u2500 configure\n            \u2514\u2500\u2500 dependencies\n                \u2514\u2500\u2500 configure-deps\n                    \u251c\u2500\u2500 Dockerfile\n                    \u251c\u2500\u2500 resources\n                    \u2502\xa0\xa0 \u2514\u2500\u2500 providers.tf\n                    \u2514\u2500\u2500 scripts\n                        \u2514\u2500\u2500 pipeline.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:["To ensure it can be used within the promise, build the ",(0,r.jsx)(n.code,{children:"kratix-workshop/s3-promise-pipeline"})," image and make it available to the cluster. If you are using kind, you can load it to your cluster, else you will need to push it to a valid image store you have push access to which may affect the name."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"docker build -t kratix-workshop/s3-promise-pipeline:v0.1.0 workflows/promise/configure/dependencies/configure-deps\nkind load docker-image kratix-workshop/s3-promise-pipeline:v0.1.0 -n platform\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This will be used within the ",(0,r.jsx)(n.code,{children:"promise.configure"})," workflow which has been added to the ",(0,r.jsx)(n.code,{children:"promise.yaml"})]}),"\n",(0,r.jsx)(n.h2,{id:"validating-the-generated-resources",children:"Validating the generated resources"}),"\n",(0,r.jsx)(n.p,{children:"Our Promise is not yet complete, but we can validate the implementation so far to ensure that it can create s3 buckets. Install the Promise by running:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"kubectl apply -f promise.yaml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Once the Promise workflow has run to completion, you will see the ",(0,r.jsx)(n.code,{children:"provider.tf"})," file in the git repository watched by your Terraform workspace."]}),"\n","\n",(0,r.jsx)("figure",{className:"diagram",children:(0,r.jsx)("img",{className:"large",src:s,alt:"A screen shot of the Github with provider.rf file present in a repository"})}),"\n",(0,r.jsx)(n.p,{children:"Ensure the Promise is Available with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$ kubectl get promise s3\n\nNAME   STATUS      KIND   API VERSION                    VERSION\ns3     Available   S3     example.syntasso.io/v1alpha1   v0.0.1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["With the Promise installed, we can make a request of it. Update the ",(0,r.jsx)(n.code,{children:"example-resource.yaml"})," to give the bucket a region:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: example.syntasso.io/v1alpha1\nkind: S3\nmetadata:\n  name: example-s3\nspec:\n  region: us-east-1\n"})}),"\n",(0,r.jsx)(n.p,{children:"Make the request with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"kubectl apply -f example-resource.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:"In the git repository you set up for your Destination, you should eventually see output very similar to the following:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'{\n  "module": {\n    "s3_default_example-s3": {\n...\n      "region": "us-east-1",\n      "restrict_public_buckets": true,\n      "skip_destroy_public_access_block": true,\n      "source": "git::https://github.com/terraform-aws-modules/terraform-aws-s3-bucket.git?ref=v5.10.0",\n      "tags": {},\n      "type": "Directory",\n      "versioning": {}\n    }\n  }\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Note that the ",(0,r.jsx)(n.code,{children:"region"}),' is "us-east-1", which comes from our Resource Request whereas all of the other values are populated by the defaults in the Promise API.']}),"\n",(0,r.jsx)(n.p,{children:"Writing this file to the repository watched by Terraform triggers an apply in the Terraform workspace and eventually, the s3 bucket is\ncreated."}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"In this workshop we're using Terraform Cloud, however, this flow is possible when using other Terraform Automation and Collaboration Software within a GipOps flow."})}),"\n",(0,r.jsx)(n.p,{children:"Take a look at the output of the Terraform run, the bucket was successfully created. But, what was it called? How can we find it?"}),"\n","\n",(0,r.jsxs)("figure",{className:"diagram",children:[(0,r.jsx)("img",{className:"large",src:a,alt:"A screen shot of the Terraform UI with the s3_bucket_bucket_regional_domain_name and s3_bucket_id values detailed as outputs"}),(0,r.jsx)("figcaption",{children:"Terraform Plan"})]}),"\n",(0,r.jsx)(n.p,{children:"As you can see from the Terraform Plan, the name of the bucket is only known after it's creation. We didn't give the bucket a name so one has been generated randomly; this is helpful to reduce duplication but isn't great for users\nwho have requested a promise. The only path to finding out is by looking in the AWS console which is inconvenient and many users will not have access to it."}),"\n",(0,r.jsx)(n.p,{children:"We can make life easier for both users and ourselves by surfacing this in the status of the resource."}),"\n",(0,r.jsx)(n.h2,{id:"surfacing-terraform-outputs",children:"Surfacing Terraform outputs"}),"\n",(0,r.jsxs)(n.p,{children:["Lets take a look at the possible outputs in the ",(0,r.jsx)(n.a,{href:"https://github.com/terraform-aws-modules/terraform-aws-s3-bucket",children:"S3 Bucket Module"}),", some useful information that we could surface is the ",(0,r.jsx)(n.code,{children:"s3_bucket_id"})," which will give\nus the bucket name as well as the ",(0,r.jsx)(n.code,{children:"s3_bucket_bucket_regional_domain_name"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Add the following container step to the ",(0,r.jsx)(n.code,{children:"instance-configure"})," pipeline."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'  - image: ghcr.io/syntasso/kratix-pipeline-utility:v0.0.1\n    name: generate-output\n    command: [ "sh" ]\n    args:\n      - -c\n      - |\n        INPUT_OBJECT_PATH="/kratix/input/object.yaml"\n        INPUT_DIR="/kratix/output"\n        OUTPUT_DIR="/kratix/output"\n\n        KIND="$(yq eval \'.kind\' "$INPUT_OBJECT_PATH" | tr \'[:upper:]\' \'[:lower:]\')"\n        NAMESPACE="$(yq eval \'.metadata.namespace\' "$INPUT_OBJECT_PATH")"\n        NAME="$(yq eval \'.metadata.name\' "$INPUT_OBJECT_PATH")"\n\n        INPUT_FILE_NAME="${KIND}_${NAMESPACE}_${NAME}.tf.json"\n        INPUT_PATH="${INPUT_DIR}/${INPUT_FILE_NAME}"\n        OUTPUT_PATH="${OUTPUT_DIR}/${INPUT_FILE_NAME}"\n\n        MODULE_NAME=$(jq -r \'.module | keys[0] // empty\' "$INPUT_PATH")\n\n        jq --arg module "$MODULE_NAME" \'\n          .output = (.output // {})\n          | .output.s3_bucket_bucket_regional_domain_name = {"value": "${module." + $module + ".s3_bucket_bucket_regional_domain_name}"}\n          | .output.s3_bucket_id = {"value": "${module." + $module + ".s3_bucket_id}"}\n        \' "$INPUT_PATH" > "${INPUT_PATH}.tmp"\n\n        mv "${INPUT_PATH}.tmp" "$OUTPUT_PATH"\n\n        printf "Updated outputs in %s\\n" "$OUTPUT_PATH"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["This script fetches the generated Terraform file and adds an ",(0,r.jsx)(n.code,{children:"output"})," block which will ensure that the ",(0,r.jsx)(n.code,{children:"s3_bucket_id"})," and\n",(0,r.jsx)(n.code,{children:"s3_bucket_bucket_regional_domain_name"})," are outputted when the resources are created."]}),"\n",(0,r.jsx)(n.p,{children:"When the Terraform apply runs, our desired outputs will be visible via the the Terraform Cloud UI but how do we ensure this information\nis visible to the user who has made the request? It's very likely that they don't have direct access to Terraform Cloud so how can this\ninformation be communicated back to them?"}),"\n",(0,r.jsxs)(n.p,{children:["This is where the ",(0,r.jsx)(n.a,{href:"/ske/integrations/tfe#adding-the-ske-tf-state-finder-pipeline-stage",children:"tfstate-finder"})," comes in. The tfstate-finder is a Pipeline Stage that fetches the outputs from the Terraform run and makes these visible in the Resource Request."]}),"\n",(0,r.jsxs)(n.p,{children:["To utilise it, we can add a second Pipeline after the ",(0,r.jsx)(n.code,{children:"instance-configure"})," Pipeline:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'  - apiVersion: platform.kratix.io/v1alpha1\n    kind: Pipeline\n    metadata:\n      name: output-writer\n    spec:\n      rbac:\n        permissions:\n        - apiGroups:\n            - ""\n          resources:\n            - secrets\n          verbs:\n            - get\n          resourceNamespace: "*"\n        - apiGroups:\n            - platform.kratix.io\n          resources:\n            - workplacements\n          verbs:\n            - list\n            - get\n        - apiGroups:\n            - platform.kratix.io\n          resources:\n            - destinations\n          verbs:\n            - list\n          resourceNamespace: "*"\n      containers:\n        - image: registry.syntasso.io/syntasso/ske-tfstate-finder:v0.7.3\n          name: fetch-output\n          env:\n          - name: TIMEOUT\n            value: "30m"\n'})}),"\n",(0,r.jsx)(n.p,{children:"As you can see, the Pipeline requires some additional RBAC permissions; this ensures that the stage can:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"List WorkPlacements to find the Destination associated with the Resource"}),"\n",(0,r.jsx)(n.li,{children:"List Destinations to find the credentials needed to access TFE"}),"\n",(0,r.jsx)(n.li,{children:"Get the Secret containing the TFE credentials"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"With the Promise now configured to produce terraform outputs and surface them in the resource status, we're ready to update our promise and see it working end-to-end."}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Click here"})," for the full ",(0,r.jsx)(n.code,{children:"promise.yaml"})," file."]})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: platform.kratix.io/v1alpha1\nkind: Promise\nmetadata:\n  creationTimestamp: null\n  labels:\n    kratix.io/promise-version: v0.0.1\n  name: s3\nspec:\n  api:\n    apiVersion: apiextensions.k8s.io/v1\n    kind: CustomResourceDefinition\n    metadata:\n      name: s3s.example.syntasso.io\n    spec:\n      group: example.syntasso.io\n      names:\n        kind: S3\n        plural: s3s\n        singular: s3\n      scope: Namespaced\n      versions:\n      - name: v1alpha1\n        schema:\n          openAPIV3Schema:\n            properties:\n              spec:\n                default: {}\n                properties:\n                  acceleration_status:\n                    description: (Optional) Sets the accelerate configuration of an\n                      existing bucket. Can be Enabled or Suspended.\n                    type: string\n                  access_log_delivery_policy_source_accounts:\n                    default: []\n                    description: (Optional) List of AWS Account IDs should be allowed\n                      to deliver access logs to this bucket.\n                    items:\n                      type: string\n                    type: array\n                  access_log_delivery_policy_source_buckets:\n                    default: []\n                    description: (Optional) List of S3 bucket ARNs which should be\n                      allowed to deliver access logs to this bucket.\n                    items:\n                      type: string\n                    type: array\n                  access_log_delivery_policy_source_organizations:\n                    default: []\n                    description: (Optional) List of AWS Organization IDs should be\n                      allowed to deliver access logs to this bucket.\n                    items:\n                      type: string\n                    type: array\n                  acl:\n                    description: (Optional) The canned ACL to apply. Conflicts with\n                      `grant`\n                    type: string\n                  allowed_kms_key_arn:\n                    description: The ARN of KMS key which should be allowed in PutObject\n                    type: string\n                  analytics_self_source_destination:\n                    default: false\n                    description: Whether or not the analytics source bucket is also\n                      the destination bucket.\n                    type: boolean\n                  analytics_source_account_id:\n                    description: The analytics source account id.\n                    type: string\n                  analytics_source_bucket_arn:\n                    description: The analytics source bucket ARN.\n                    type: string\n                  attach_access_log_delivery_policy:\n                    default: false\n                    description: Controls if S3 bucket should have S3 access log delivery\n                      policy attached\n                    type: boolean\n                  attach_analytics_destination_policy:\n                    default: false\n                    description: Controls if S3 bucket should have bucket analytics\n                      destination policy attached.\n                    type: boolean\n                  attach_cloudtrail_log_delivery_policy:\n                    default: false\n                    description: Controls if S3 bucket should have CloudTrail log\n                      delivery policy attached\n                    type: boolean\n                  attach_deny_incorrect_encryption_headers:\n                    default: false\n                    description: Controls if S3 bucket should deny incorrect encryption\n                      headers policy attached.\n                    type: boolean\n                  attach_deny_incorrect_kms_key_sse:\n                    default: false\n                    description: Controls if S3 bucket policy should deny usage of\n                      incorrect KMS key SSE.\n                    type: boolean\n                  attach_deny_insecure_transport_policy:\n                    default: false\n                    description: Controls if S3 bucket should have deny non-SSL transport\n                      policy attached\n                    type: boolean\n                  attach_deny_ssec_encrypted_object_uploads:\n                    default: false\n                    description: Controls if S3 bucket should deny SSEC encrypted\n                      object uploads.\n                    type: boolean\n                  attach_deny_unencrypted_object_uploads:\n                    default: false\n                    description: Controls if S3 bucket should deny unencrypted object\n                      uploads policy attached.\n                    type: boolean\n                  attach_elb_log_delivery_policy:\n                    default: false\n                    description: Controls if S3 bucket should have ELB log delivery\n                      policy attached\n                    type: boolean\n                  attach_inventory_destination_policy:\n                    default: false\n                    description: Controls if S3 bucket should have bucket inventory\n                      destination policy attached.\n                    type: boolean\n                  attach_lb_log_delivery_policy:\n                    default: false\n                    description: Controls if S3 bucket should have ALB/NLB log delivery\n                      policy attached\n                    type: boolean\n                  attach_policy:\n                    default: false\n                    description: Controls if S3 bucket should have bucket policy attached\n                      (set to `true` to use value of `policy` as bucket policy)\n                    type: boolean\n                  attach_public_policy:\n                    default: true\n                    description: Controls if a user defined public bucket policy will\n                      be attached (set to `false` to allow upstream to apply defaults\n                      to the bucket)\n                    type: boolean\n                  attach_require_latest_tls_policy:\n                    default: false\n                    description: Controls if S3 bucket should require the latest version\n                      of TLS\n                    type: boolean\n                  attach_waf_log_delivery_policy:\n                    default: false\n                    description: Controls if S3 bucket should have WAF log delivery\n                      policy attached\n                    type: boolean\n                  availability_zone_id:\n                    description: Availability Zone ID or Local Zone ID\n                    type: string\n                  block_public_acls:\n                    default: true\n                    description: Whether Amazon S3 should block public ACLs for this\n                      bucket.\n                    type: boolean\n                  block_public_policy:\n                    default: true\n                    description: Whether Amazon S3 should block public bucket policies\n                      for this bucket.\n                    type: boolean\n                  bucket:\n                    description: (Optional, Forces new resource) The name of the bucket.\n                      If omitted, Terraform will assign a random, unique name.\n                    type: string\n                  bucket_prefix:\n                    description: (Optional, Forces new resource) Creates a unique\n                      bucket name beginning with the specified prefix. Conflicts with\n                      bucket.\n                    type: string\n                  control_object_ownership:\n                    default: false\n                    description: Whether to manage S3 Bucket Ownership Controls on\n                      this bucket.\n                    type: boolean\n                  create_bucket:\n                    default: true\n                    description: Controls if S3 bucket should be created\n                    type: boolean\n                  create_metadata_configuration:\n                    default: false\n                    description: Whether to create metadata configuration resource\n                    type: boolean\n                  data_redundancy:\n                    description: \'Data redundancy. Valid values: `SingleAvailabilityZone`\'\n                    type: string\n                  expected_bucket_owner:\n                    description: The account ID of the expected bucket owner\n                    type: string\n                  force_destroy:\n                    default: false\n                    description: (Optional, Default:false ) A boolean that indicates\n                      all objects should be deleted from the bucket so that the bucket\n                      can be destroyed without error. These objects are not recoverable.\n                    type: boolean\n                  ignore_public_acls:\n                    default: true\n                    description: Whether Amazon S3 should ignore public ACLs for this\n                      bucket.\n                    type: boolean\n                  inventory_self_source_destination:\n                    default: false\n                    description: Whether or not the inventory source bucket is also\n                      the destination bucket.\n                    type: boolean\n                  inventory_source_account_id:\n                    description: The inventory source account id.\n                    type: string\n                  inventory_source_bucket_arn:\n                    description: The inventory source bucket ARN.\n                    type: string\n                  is_directory_bucket:\n                    default: false\n                    description: If the s3 bucket created is a directory bucket\n                    type: boolean\n                  lb_log_delivery_policy_source_organizations:\n                    default: []\n                    description: (Optional) List of AWS Organization IDs should be\n                      allowed to deliver ALB/NLB logs to this bucket.\n                    items:\n                      type: string\n                    type: array\n                  location_type:\n                    description: \'Location type. Valid values: `AvailabilityZone`\n                      or `LocalZone`\'\n                    type: string\n                  metadata_inventory_table_configuration_state:\n                    description: \'Configuration state of the inventory table, indicating\n                      whether the inventory table is enabled or disabled. Valid values:\n                      ENABLED, DISABLED\'\n                    type: string\n                  metadata_journal_table_record_expiration:\n                    description: \'Whether journal table record expiration is enabled\n                      or disabled. Valid values: ENABLED, DISABLED\'\n                    type: string\n                  metadata_journal_table_record_expiration_days:\n                    description: Number of days to retain journal table records\n                    type: number\n                  object_lock_enabled:\n                    default: false\n                    description: Whether S3 bucket should have an Object Lock configuration\n                      enabled.\n                    type: boolean\n                  object_ownership:\n                    default: BucketOwnerEnforced\n                    description: \'Object ownership. Valid values: BucketOwnerEnforced,\n                      BucketOwnerPreferred or ObjectWriter. \'\'BucketOwnerEnforced\'\':\n                      ACLs are disabled, and the bucket owner automatically owns and\n                      has full control over every object in the bucket. \'\'BucketOwnerPreferred\'\':\n                      Objects uploaded to the bucket change ownership to the bucket\n                      owner if the objects are uploaded with the bucket-owner-full-control\n                      canned ACL. \'\'ObjectWriter\'\': The uploading account will own\n                      the object if the object is uploaded with the bucket-owner-full-control\n                      canned ACL.\'\n                    type: string\n                  owner:\n                    additionalProperties:\n                      type: string\n                    default: {}\n                    description: Bucket owner\'s display name and ID. Conflicts with\n                      `acl`\n                    type: object\n                  policy:\n                    description: (Optional) A valid bucket policy JSON document. Note\n                      that if the policy document is not specific enough (but still\n                      valid), Terraform may view the policy as constantly changing\n                      in a terraform plan. In this case, please make sure you use\n                      the verbose/specific version of the policy. For more information\n                      about building AWS IAM policy documents with Terraform, see\n                      the AWS IAM Policy Document Guide.\n                    type: string\n                  putin_khuylo:\n                    default: true\n                    description: \'Do you agree that Putin doesn\'\'t respect Ukrainian\n                      sovereignty and territorial integrity? More info: https://en.wikipedia.org/wiki/Putin_khuylo!\'\n                    type: boolean\n                  region:\n                    description: Region where the resource(s) will be managed. Defaults\n                      to the region set in the provider configuration\n                    type: string\n                  request_payer:\n                    description: (Optional) Specifies who should bear the cost of\n                      Amazon S3 data transfer. Can be either BucketOwner or Requester.\n                      By default, the owner of the S3 bucket would incur the costs\n                      of any data transfer. See Requester Pays Buckets developer guide\n                      for more information.\n                    type: string\n                  restrict_public_buckets:\n                    default: true\n                    description: Whether Amazon S3 should restrict public bucket policies\n                      for this bucket.\n                    type: boolean\n                  skip_destroy_public_access_block:\n                    default: true\n                    description: Whether to skip destroying the S3 Bucket Public Access\n                      Block configuration when destroying the bucket. Only used if\n                      `public_access_block` is set to true.\n                    type: boolean\n                  tags:\n                    additionalProperties:\n                      type: string\n                    default: {}\n                    description: (Optional) A mapping of tags to assign to the bucket.\n                    type: object\n                  transition_default_minimum_object_size:\n                    description: \'The default minimum object size behavior applied\n                      to the lifecycle configuration. Valid values: all_storage_classes_128K\n                      (default), varies_by_storage_class\'\n                    type: string\n                  type:\n                    default: Directory\n                    description: \'Bucket type. Valid values: `Directory`\'\n                    type: string\n                  versioning:\n                    additionalProperties:\n                      type: string\n                    default: {}\n                    description: Map containing versioning configuration.\n                    type: object\n                type: object\n            type: object\n        served: true\n        storage: true\n  destinationSelectors:\n  - matchLabels:\n      environment: terraform\n  workflows:\n    config: {}\n    promise:\n      configure:\n      - apiVersion: platform.kratix.io/v1alpha1\n        kind: Pipeline\n        metadata:\n          creationTimestamp: null\n          name: dependencies\n        spec:\n          containers:\n          - image: kratix-workshop/s3-promise-pipeline:v0.1.0\n            name: configure-deps\n          jobOptions: {}\n          rbac: {}\n    resource:\n      configure:\n      - apiVersion: platform.kratix.io/v1alpha1\n        kind: Pipeline\n        metadata:\n          name: instance-configure\n        spec:\n          containers:\n          - env:\n            - name: MODULE_SOURCE\n              value: git::https://github.com/terraform-aws-modules/terraform-aws-s3-bucket.git?ref=v5.10.0\n            image: ghcr.io/syntasso/kratix-cli/terraform-generate:v0.4.0\n            name: terraform-generate\n          - image: ghcr.io/syntasso/kratix-pipeline-utility:v0.0.1\n            name: generate-output\n            command: [ "sh" ]\n            args:\n              - -c\n              - |\n                INPUT_OBJECT_PATH="/kratix/input/object.yaml"\n                INPUT_DIR="/kratix/output"\n                OUTPUT_DIR="/kratix/output"\n\n                KIND="$(yq eval \'.kind\' "$INPUT_OBJECT_PATH" | tr \'[:upper:]\' \'[:lower:]\')"\n                NAMESPACE="$(yq eval \'.metadata.namespace\' "$INPUT_OBJECT_PATH")"\n                NAME="$(yq eval \'.metadata.name\' "$INPUT_OBJECT_PATH")"\n\n                INPUT_FILE_NAME="${KIND}_${NAMESPACE}_${NAME}.tf.json"\n                INPUT_PATH="${INPUT_DIR}/${INPUT_FILE_NAME}"\n                OUTPUT_PATH="${OUTPUT_DIR}/${INPUT_FILE_NAME}"\n\n                MODULE_NAME=$(jq -r \'.module | keys[0] // empty\' "$INPUT_PATH")\n\n                jq --arg module "$MODULE_NAME" \'\n                  .output = (.output // {})\n                  | .output.s3_bucket_bucket_regional_domain_name = {"value": "${module." + $module + ".s3_bucket_bucket_regional_domain_name}"}\n                  | .output.s3_bucket_id = {"value": "${module." + $module + ".s3_bucket_id}"}\n                \' "$INPUT_PATH" > "${INPUT_PATH}.tmp"\n\n                mv "${INPUT_PATH}.tmp" "$OUTPUT_PATH"\n\n                printf "Updated outputs in %s\\n" "$OUTPUT_PATH"   \n      - apiVersion: platform.kratix.io/v1alpha1\n        kind: Pipeline\n        metadata:\n          name: output-writer\n        spec:\n          rbac:\n            permissions:\n            - apiGroups:\n                - ""\n              resources:\n                - secrets\n              verbs:\n                - get\n              resourceNamespace: "*"\n            - apiGroups:\n                - platform.kratix.io\n              resources:\n                - workplacements\n              verbs:\n                - list\n                - get\n            - apiGroups:\n                - platform.kratix.io\n              resources:\n                - destinations\n              verbs:\n                - list\n              resourceNamespace: "*"\n          containers:\n            - image: registry.syntasso.io/syntasso/ske-tfstate-finder:v0.7.3\n              name: fetch-output\n              env:\n              - name: TIMEOUT\n                value: "30m"\nstatus:\n  workflows: 0\n  workflowsFailed: 0\n  workflowsSucceeded: 0\n'})})]}),"\n",(0,r.jsx)(n.h3,{id:"validating-the-completed-promise",children:"Validating the completed Promise"}),"\n",(0,r.jsx)(n.p,{children:"Apply the promise once again with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"kubectl apply -f promise.yaml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This will re-trigger both the Promise workflows and the Resource workflows. When the workflows have completed, you should see that another Terraform apply has been triggered and this time, it details our desired outputs. Take a look at the Terraform Cloud run, this shows that there were two outputs, the ",(0,r.jsx)(n.code,{children:"s3_bucket_bucket_regional_domain_name"})," and the ",(0,r.jsx)(n.code,{children:"s3_bucket_id"}),":"]}),"\n","\n",(0,r.jsxs)("figure",{className:"diagram",children:[(0,r.jsx)("img",{className:"large",src:l,alt:"A screen shot of the Terraform UI with the s3_bucket_bucket_regional_domain_name and s3_bucket_id values detailed as outputs"}),(0,r.jsx)("figcaption",{children:"Terraform Outputs"})]}),"\n",(0,r.jsx)(n.p,{children:"And now, let's look at the Resource Request itself:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl describe s3 example-s3\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You should see that the ",(0,r.jsx)(n.code,{children:"status"})," of the request now mirrors the outputs we saw in the Terraform Cloud UI:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"  Last Successful Configure Workflow Time:  2026-01-14T12:06:26Z\n  Message:                                  Resource requested\n  Observed Generation:                      1\n#highlight-start\n  Outputs:\n    Tfstate:\n      s3_bucket_bucket_regional_domain_name:  terraform-20260114120617401500000001.s3.us-east-1.amazonaws.com\n      s3_bucket_id:                           terraform-20260114120617401500000001\n#highlight-end\n  Workflows:                                  2\n  Workflows Failed:                           0\n  Workflows Succeeded:                        2\n"})}),"\n",(0,r.jsx)(n.h2,{id:"-congratulations",children:"\ud83c\udf89 Congratulations"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705\xa0\xa0 You have successfully created a Promise from a Terraform module! It can create infrastructure based on inputs from requests, produce outputs and surface these to users in the status of the request.",(0,r.jsx)("br",{})]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}}}]);